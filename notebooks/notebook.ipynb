{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "import jax\n",
    "from jax import grad, jit\n",
    "import jax.numpy as jnp\n",
    "import random\n",
    "import haiku as hk\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from src.utils import get_args_and_config\n",
    "from src.data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "args, _ = get_args_and_config()\n",
    "data = load_data(args) if 'data' not in locals() else eval('data')\n",
    "train_data = {subject: data[subject]['folds'] for subject in data.keys()}\n",
    "test_data = {subject: data[subject]['test'] for subject in data.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypere param sweep config in config\n",
    "config = dict(\n",
    "    embed_size=1024,\n",
    "    hidden_dim=1024,\n",
    "    n_layers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_fn(fmri, config):\n",
    "    \"\"\"embedding function\"\"\"\n",
    "    n_layers = config['n_layers']\n",
    "    embed_size = config['embed_size']\n",
    "    img_mlp = hk.Sequential([\n",
    "        # linear layer to get embedding without bias\n",
    "        hk.Linear(embed_size, with_bias=False),\n",
    "        jax.nn.tanh,\n",
    "    ])\n",
    "    return img_mlp(fmri)\n",
    "\n",
    "def decoder_fn(z, config):\n",
    "    \"\"\"decoder function\"\"\"\n",
    "    n_layers = config['n_layers']\n",
    "    hidden_dim = config['hidden_dim']\n",
    "    embed_size = config['embed_size']\n",
    "    img_mlp = hk.Sequential([\n",
    "        hk.Linear(hidden_dim, with_bias=False),\n",
    "        jax.nn.tanh,\n",
    "    ])\n",
    "    img_deconv = hk.Sequential([\n",
    "        hk.Conv2DTranspose(3, kernel_shape=4, stride=2, padding='SAME'),\n",
    "        jax.nn.sigmoid,\n",
    "        # go from 64x64x3 to to 224x224x3\n",
    "        hk.Conv2DTranspose(3, kernel_shape=4, stride=2, padding='SAME'),\n",
    "        jax.nn.sigmoid,\n",
    "    ])\n",
    "    z = img_mlp(z)\n",
    "    z = z.reshape((-1, int(hidden_dim ** 0.5), int(hidden_dim ** 0.5), 1))\n",
    "    z = img_deconv(z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_fn = partial(embedding_fn, config=config)\n",
    "decoder_fn = partial(decoder_fn, config=config)\n",
    "init_embed, apply_embed = hk.without_apply_rng(hk.transform(embedding_fn))\n",
    "init_decoder, apply_decoder = hk.without_apply_rng(hk.transform(decoder_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss_fn(params, lh, rh, img):\n",
    "    \"\"\"loss function\"\"\"\n",
    "    lh_embed = apply_embed(params[0], lh)\n",
    "    rh_embed = apply_embed(params[1], rh)\n",
    "    embed = jnp.concatenate([lh_embed, rh_embed], axis=-1)\n",
    "    img_hat = apply_decoder(params[2], embed)\n",
    "    return jnp.mean((img - img_hat) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(params, opt_state, train_batches):\n",
    "    \"\"\"update function\"\"\"\n",
    "    grads = grad(loss_fn)(params, x1, x2, y)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data_split, batch_size=32):\n",
    "    \"\"\"get a batch of data\"\"\"\n",
    "    while True:\n",
    "        fold_perm = np.random.permutation(len(data_split['subj01']))  # either 5 or 1 (train or val)\n",
    "        for fold_idx in fold_perm:\n",
    "            subject_folds = {key: value[fold_idx] for key, value in train_data.items()}\n",
    "            n_samples = min([len(value[0]) for value in subject_folds.values()])\n",
    "            sample_perm = np.random.permutation(n_samples)\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                for subject in subject_folds.keys():\n",
    "                    batch_data = (subject_folds[subject][0][sample_perm[i:i+batch_size]],\n",
    "                                  subject_folds[subject][1][sample_perm[i:i+batch_size]],\n",
    "                                  subject_folds[subject][2][sample_perm[i:i+batch_size]])\n",
    "                    batch_data_gpu = jax.device_put(batch_data)\n",
    "                    yield batch_data_gpu\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = get_batches(train_data)\n",
    "dummy_batches = [next(train_batches) for _ in range(len(train_data))]\n",
    "lh_embed_params = [init_embed(jax.random.PRNGKey(42), batch[0]) for batch in dummy_batches]\n",
    "rh_embed_params = [init_embed(jax.random.PRNGKey(42), batch[1]) for batch in dummy_batches]\n",
    "decoder_params = init_decoder(jax.random.PRNGKey(42), jnp.zeros((1, config['embed_size'] * 2)))\n",
    "params = (lh_embed_params, rh_embed_params, decoder_params)\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "params argument does not appear valid. It should be a mapping but is of type <class 'list'>. For reference the parameters for apply are `apply(params, rng, ...)`` for `hk.transform` and `apply(params, state, rng, ...)` for `hk.transform_with_state`.\nThe argument was: [{'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[18978,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[18981,1024])>with<DynamicJaxprTrace(level=4/0)>}}].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lens \u001b[39m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m((\u001b[39m10_000\u001b[39m\u001b[39m*\u001b[39m\u001b[39m8\u001b[39m\u001b[39m*\u001b[39m\u001b[39m10\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m  \u001b[39m32\u001b[39m)):\n\u001b[0;32m----> 3\u001b[0m     params, opt_state \u001b[39m=\u001b[39m update(params, opt_state, \u001b[39m*\u001b[39;49m\u001b[39mnext\u001b[39;49m(train_batches))\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m100\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m      5\u001b[0m         loss \u001b[39m=\u001b[39m loss_fn(params, \u001b[39m*\u001b[39m\u001b[39mnext\u001b[39m(train_batches))\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[171], line 4\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(params, opt_state, x1, x2, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m@jit\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(params, opt_state, x1, x2, y):\n\u001b[1;32m      3\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"update function\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     grads \u001b[39m=\u001b[39m grad(loss_fn)(params, x1, x2, y)\n\u001b[1;32m      5\u001b[0m     updates, opt_state \u001b[39m=\u001b[39m opt\u001b[39m.\u001b[39mupdate(grads, opt_state, params)\n\u001b[1;32m      6\u001b[0m     new_params \u001b[39m=\u001b[39m optax\u001b[39m.\u001b[39mapply_updates(params, updates)\n",
      "    \u001b[0;31m[... skipping hidden 22 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[170], line 4\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(params, lh, rh, img)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m@jit\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss_fn\u001b[39m(params, lh, rh, img):\n\u001b[1;32m      3\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"loss function\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     lh_embed \u001b[39m=\u001b[39m apply_embed(params[\u001b[39m0\u001b[39;49m], lh)\n\u001b[1;32m      5\u001b[0m     rh_embed \u001b[39m=\u001b[39m apply_embed(params[\u001b[39m1\u001b[39m], rh)\n\u001b[1;32m      6\u001b[0m     embed \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mconcatenate([lh_embed, rh_embed], axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/haiku/_src/multi_transform.py:298\u001b[0m, in \u001b[0;36mwithout_apply_rng.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_fn\u001b[39m(params, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    297\u001b[0m   check_rng_kwarg(kwargs)\n\u001b[0;32m--> 298\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39;49mapply(params, \u001b[39mNone\u001b[39;49;00m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/haiku/_src/transform.py:128\u001b[0m, in \u001b[0;36mwithout_state.<locals>.apply_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    122\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    123\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mHaiku transform adds three arguments (params, state, rng) to apply. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mIf the functions you are transforming use the same names you must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mpass them positionally (e.g. `f.apply(.., my_state)` and not by \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mname (e.g. `f.apply(.., state=my_state)`)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 128\u001b[0m out, state \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mapply(params, {}, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    129\u001b[0m \u001b[39mif\u001b[39;00m state:\n\u001b[1;32m    130\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf your transformed function uses `hk.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mget,set}_state` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mthen use `hk.transform_with_state`.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/haiku/_src/transform.py:351\u001b[0m, in \u001b[0;36mtransform_with_state.<locals>.apply_fn\u001b[0;34m(params, state, rng, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_fn\u001b[39m(\n\u001b[1;32m    344\u001b[0m     params: Optional[hk\u001b[39m.\u001b[39mParams],\n\u001b[1;32m    345\u001b[0m     state: Optional[hk\u001b[39m.\u001b[39mState],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    349\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Any, hk\u001b[39m.\u001b[39mState]:\n\u001b[1;32m    350\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Applies your function injecting parameters and state.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m   params \u001b[39m=\u001b[39m check_mapping(\u001b[39m\"\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m\"\u001b[39;49m, params)\n\u001b[1;32m    352\u001b[0m   state \u001b[39m=\u001b[39m check_mapping(\u001b[39m\"\u001b[39m\u001b[39mstate\u001b[39m\u001b[39m\"\u001b[39m, state)\n\u001b[1;32m    353\u001b[0m   rng \u001b[39m=\u001b[39m to_prng_sequence(\n\u001b[1;32m    354\u001b[0m       rng, err_msg\u001b[39m=\u001b[39m(APPLY_RNG_STATE_ERROR \u001b[39mif\u001b[39;00m state \u001b[39melse\u001b[39;00m APPLY_RNG_ERROR))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/haiku/_src/transform.py:394\u001b[0m, in \u001b[0;36mcheck_mapping\u001b[0;34m(name, mapping)\u001b[0m\n\u001b[1;32m    389\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(mapping)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_DictWrapper\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    390\u001b[0m     \u001b[39m# TensorFlow's checkpointing infrastructure replaces `dict` instances on\u001b[39;00m\n\u001b[1;32m    391\u001b[0m     \u001b[39m# `tf.Module`s with a type that is not a `Mapping` instance.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping\n\u001b[0;32m--> 394\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m argument does not appear valid. It should be a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmapping but is of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(mapping)\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mFor reference the parameters for apply are \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    397\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`apply(params, rng, ...)`` for `hk.transform` and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    398\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`apply(params, state, rng, ...)` for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`hk.transform_with_state`.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe argument was: \u001b[39m\u001b[39m{\u001b[39;00mmapping\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    401\u001b[0m \u001b[39mreturn\u001b[39;00m mapping\n",
      "\u001b[0;31mTypeError\u001b[0m: params argument does not appear valid. It should be a mapping but is of type <class 'list'>. For reference the parameters for apply are `apply(params, rng, ...)`` for `hk.transform` and `apply(params, state, rng, ...)` for `hk.transform_with_state`.\nThe argument was: [{'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[18978,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[19004,1024])>with<DynamicJaxprTrace(level=4/0)>}}, {'linear': {'w': Traced<ShapedArray(float32[18981,1024])>with<DynamicJaxprTrace(level=4/0)>}}]."
     ]
    }
   ],
   "source": [
    "lens = []\n",
    "for i in tqdm(range((10_000*8*10) //  32)):\n",
    "    params, opt_state = update(params, opt_state, train_batches)\n",
    "    if i % 100 == 0:\n",
    "        loss = loss_fn(params, *next(train_batches))\n",
    "        lens.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
