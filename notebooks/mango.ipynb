{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroscape playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "import optax\n",
    "from tqdm import tqdm\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import wandb\n",
    "from functools import partial\n",
    "from src.data import get_data\n",
    "from src.utils import get_args_and_config\n",
    "from src.fmri import plot_brain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MANGO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args, _ = get_args_and_config()\n",
    "data = get_data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optax.adamw(0.001)  # perhaps hyper param search for lr and weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    x_mlp = hk.Sequential([\n",
    "        hk.nets.MLP([100] * 1, activation=jnp.tanh),\n",
    "        hk.Linear(100),\n",
    "    ])\n",
    "    lh_ml = hk.Sequential([\n",
    "        hk.nets.MLP([100] * 1, activation=jnp.tanh),\n",
    "        hk.Linear(19004),\n",
    "    ])\n",
    "    rh_ml = hk.Sequential([\n",
    "        hk.nets.MLP([100] * 1, activation=jnp.tanh),\n",
    "        hk.Linear(20544),\n",
    "    ])\n",
    "    x = x_mlp(x)\n",
    "    lh_hat = lh_ml(x)\n",
    "    rh_hat = rh_ml(x)\n",
    "    return lh_hat, rh_hat\n",
    "\n",
    "init_fn, apply_fn = hk.without_apply_rng(hk.transform(forward))\n",
    "\n",
    "\n",
    "def loss_fn(params, batch, hem):\n",
    "    x, lh, rh = batch\n",
    "    lh_hat, rh_hat = apply_fn(params, x)\n",
    "    return jnp.mean((lh_hat - lh) ** 2) if hem == 'lh' else jnp.mean((rh_hat - rh) ** 2)\n",
    "\n",
    "lh_loss_fn = jit(partial(loss_fn, hem='lh'))\n",
    "rh_loss_fn = jit(partial(loss_fn, hem='rh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fold(fold, fold_idx):\n",
    "    train_data = [fold for idx, fold in enumerate(fold) if idx != fold_idx]\n",
    "    train_data = list(map(jnp.vstack, zip(*train_data)))\n",
    "    val_data = fold[fold_idx]\n",
    "    return train_data, val_data\n",
    "\n",
    "def get_batch(data, batch_size):\n",
    "    while True:\n",
    "        perm = np.random.permutation(data[0].shape[0])\n",
    "        for i in range(0, data[0].shape[0], batch_size):\n",
    "            idx = perm[i:i + batch_size]\n",
    "            # x = data[0][idx]\n",
    "            x = jnp.concatenate([data[0][idx], data[3][idx]], axis=1)\n",
    "            lh = data[1][idx]\n",
    "            rh = data[2][idx]\n",
    "            yield x, lh, rh\n",
    "            \n",
    "def train(model, data, config):\n",
    "    group = wandb.util.generate_id()\n",
    "    for subject, (folds, test_data) in data.items():\n",
    "        train_data = list(map(jnp.vstack, zip(*folds)))\n",
    "        with wandb.init(project=\"neuroscope\", entity='syrkis', group=group) as run:\n",
    "            train_fold(model, train_data, test_data, config)\n",
    "\n",
    "def train_fold(model, train_data, val_data, config):\n",
    "    lh_params = init_fn(jax.random.PRNGKey(42), jnp.ones((1, 180)))\n",
    "    rh_params = init_fn(jax.random.PRNGKey(42), jnp.ones((1, 180)))\n",
    "    lh_opt_state = opt.init(lh_params)\n",
    "    rh_opt_state = opt.init(rh_params)\n",
    "    train_batches = get_batch(train_data, config['batch_size'])\n",
    "    val_batches = get_batch(val_data, config['batch_size'])\n",
    "    for step in tqdm(range(config['n_steps'])):\n",
    "        train_batch = next(train_batches)\n",
    "        lh_params, lh_opt_state = lh_update(lh_params, train_batch, lh_opt_state)\n",
    "        rh_params, rh_opt_state = rh_update(rh_params, train_batch, rh_opt_state)\n",
    "        if step % (config['n_steps'] // 100) == 0:\n",
    "            metrics = evaluate(lh_params, rh_params, train_batches, val_batches)\n",
    "            wandb.log(metrics)\n",
    "    metrics = evaluate(lh_params, rh_params, train_batches, val_batches, steps=50)\n",
    "    wandb.finish()\n",
    "\n",
    "def evaluate(lh_params, rh_params, train_batches, val_batches, steps=3):\n",
    "    train_metrics = evaluate_fold(lh_params, rh_params, train_batches, steps)\n",
    "    val_metrics = evaluate_fold(lh_params, rh_params, val_batches, steps)\n",
    "    metrics = {f'train_{k}': v for k, v in train_metrics.items()}\n",
    "    metrics.update({f'val_{k}': v for k, v in val_metrics.items()})\n",
    "    return metrics\n",
    "\n",
    "def evaluate_fold(lh_params, rh_params, batches, steps):\n",
    "    metrics = {}\n",
    "    for i in range(steps):\n",
    "        batch = next(batches)\n",
    "        batch_metrics = evaluate_batch(lh_params, rh_params, batch)\n",
    "        metrics = {k: metrics.get(k, 0) + v for k, v in batch_metrics.items()}\n",
    "    metrics = {k: v / steps for k, v in metrics.items()}\n",
    "    return metrics\n",
    "    \n",
    "\n",
    "def evaluate_batch(lh_params, rh_params, batch):\n",
    "    metrics = {}\n",
    "    for hem, params in zip(['lh', 'rh'], [lh_params, rh_params]):\n",
    "        mse, corr = evaluate_hem(params, batch, hem)\n",
    "        metrics[f'{hem}_mse'] = mse\n",
    "        metrics[f'{hem}_corr'] = corr\n",
    "    return metrics\n",
    "\n",
    "def evaluate_hem(params, batch, hem):\n",
    "    x, lh, rh = batch\n",
    "    lh_hat, rh_hat = apply_fn(params, x)\n",
    "    mse = jnp.mean((lh_hat - lh) ** 2) if hem == 'lh' else jnp.mean((rh_hat - rh) ** 2)\n",
    "    # compute the median collumn wise correlation\n",
    "    corr = pearsonr(lh_hat, lh) if hem == 'lh' else pearsonr(rh_hat, rh)\n",
    "    return mse, jnp.median(corr)\n",
    "\n",
    "\n",
    "# function for computing pearson's correlation coefficient for each voxel of a subject's fMRI data\n",
    "def pearsonr(pred, target):\n",
    "    def _pearsonr(x, y):\n",
    "        corr = jnp.corrcoef(x, y)\n",
    "        return corr[0, 1]\n",
    "    hem_corr = vmap(_pearsonr)(pred.T, target.T)\n",
    "    return hem_corr\n",
    "\n",
    "\n",
    "\n",
    "def update(params, batch, opt_state, hem):\n",
    "    \"\"\"update function\"\"\"\n",
    "    loss_fn = lh_loss_fn if hem == 'lh' else rh_loss_fn\n",
    "    grads = grad(loss_fn)(params, batch)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, opt_state\n",
    "\n",
    "lh_update = jit(partial(update, hem='lh'))\n",
    "rh_update = jit(partial(update, hem='rh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = { 'n_steps': 6000, 'batch_size': 32 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(apply_fn, data, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
