
@online{denton_deep_2015,
	title = {Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks},
	url = {https://arxiv.org/abs/1506.05751v1},
	abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets ({GAN}) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our {CIFAR}10 samples were mistaken for real images around 40\% of the time, compared to 10\% for samples drawn from a {GAN} baseline model. We also show samples from models trained on the higher resolution images of the {LSUN} scene dataset.},
	titleaddon = {{arXiv}.org},
	author = {Denton, Emily and Chintala, Soumith and Szlam, Arthur and Fergus, Rob},
	urldate = {2023-05-08},
	date = {2015-06-18},
	langid = {english},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/GJV4DRWU/Denton et al. - 2015 - Deep Generative Image Models using a Laplacian Pyr.pdf:application/pdf},
}

@article{han_variational_2019,
	title = {Variational Autoencoder: An Unsupervised Model for Encoding and Decoding {fMRI} Activity in Visual Cortex},
	volume = {198},
	issn = {1053-8119},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6592726/},
	doi = {10.1016/j.neuroimage.2019.05.039},
	shorttitle = {Variational Autoencoder},
	abstract = {Goal-driven and feedforward-only convolutional neural networks ({CNN}) have been shown to be able to predict and decode cortical responses to natural images or videos. Here, we explored an alternative deep neural network, variational auto-encoder ({VAE}), as a computational model of the visual cortex. We trained a {VAE} with a five-layer encoder and a five-layer decoder to learn visual representations from a diverse set of unlabeled images. Using the trained {VAE}, we predicted and decoded cortical activity observed with functional magnetic resonance imaging ({fMRI}) from three human subjects passively watching natural videos. Compared to {CNN}, {VAE} could predict the video-evoked cortical responses with comparable accuracy in early visual areas, but relatively lower accuracy in higher-order visual areas. The distinction between {CNN} and {VAE} in terms of encoding performance was primarily attributed to their different learning objectives, rather than their different model architecture or number of parameters. Despite lower encoding accuracies, {VAE} offered a more convenient strategy for decoding the {fMRI} activity to reconstruct the video input, by first converting the {fMRI} activity to the {VAE}’s latent variables, and then converting the latent variables to the reconstructed video frames through the {VAE}’s decoder. This strategy was more advantageous than alternative decoding methods, e.g. partial least square regression, for being able to reconstruct both the spatial structure and color of the visual input. Such findings highlight {VAE} as an unsupervised model for learning visual representation, as well as its potential and limitations for explaining cortical responses and reconstructing naturalistic and diverse visual experiences.},
	pages = {125--136},
	journaltitle = {{NeuroImage}},
	shortjournal = {Neuroimage},
	author = {Han, Kuan and Wen, Haiguang and Shi, Junxing and Lu, Kun-Han and Zhang, Yizhen and Fu, Di and Liu, Zhongming},
	urldate = {2023-05-08},
	date = {2019-09},
	pmid = {31103784},
	pmcid = {PMC6592726},
	file = {PubMed Central Full Text PDF:/Users/syrkis/Zotero/storage/ZUX9HNVT/Han et al. - 2019 - Variational Autoencoder An Unsupervised Model for.pdf:application/pdf},
}

@misc{lin_microsoft_2015,
	title = {Microsoft {COCO}: Common Objects in Context},
	url = {http://arxiv.org/abs/1405.0312},
	shorttitle = {Microsoft {COCO}},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to {PASCAL}, {ImageNet}, and {SUN}. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	number = {{arXiv}:1405.0312},
	publisher = {{arXiv}},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	urldate = {2023-05-08},
	date = {2015-02-20},
	eprinttype = {arxiv},
	eprint = {1405.0312 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/syrkis/Zotero/storage/LFKJXTSD/1405.html:text/html;Full Text PDF:/Users/syrkis/Zotero/storage/99JHN779/Lin et al. - 2015 - Microsoft COCO Common Objects in Context.pdf:application/pdf},
}

@article{shen_deep_2019,
	title = {Deep image reconstruction from human brain activity},
	volume = {15},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006633},
	doi = {10.1371/journal.pcbi.1006633},
	abstract = {The mental contents of perception and imagery are thought to be encoded in hierarchical representations in the brain, but previous attempts to visualize perceptual contents have failed to capitalize on multiple levels of the hierarchy, leaving it challenging to reconstruct internal imagery. Recent work showed that visual cortical activity measured by functional magnetic resonance imaging ({fMRI}) can be decoded (translated) into the hierarchical features of a pre-trained deep neural network ({DNN}) for the same input image, providing a way to make use of the information from hierarchical visual features. Here, we present a novel image reconstruction method, in which the pixel values of an image are optimized to make its {DNN} features similar to those decoded from human brain activity at multiple layers. We found that our method was able to reliably produce reconstructions that resembled the viewed natural images. A natural image prior introduced by a deep generator neural network effectively rendered semantically meaningful details to the reconstructions. Human judgment of the reconstructions supported the effectiveness of combining multiple {DNN} layers to enhance the visual quality of generated images. While our model was solely trained with natural images, it successfully generalized to artificial shapes, indicating that our model was not simply matching to exemplars. The same analysis applied to mental imagery demonstrated rudimentary reconstructions of the subjective content. Our results suggest that our method can effectively combine hierarchical neural representations to reconstruct perceptual and subjective images, providing a new window into the internal contents of the brain.},
	pages = {e1006633},
	number = {1},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Shen, Guohua and Horikawa, Tomoyasu and Majima, Kei and Kamitani, Yukiyasu},
	urldate = {2023-05-08},
	date = {2019-01-14},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Functional magnetic resonance imaging, Imaging techniques, Luminance, Neural networks, Optimization, Sensory perception, Vision},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/WUES9AEV/Shen et al. - 2019 - Deep image reconstruction from human brain activit.pdf:application/pdf},
}

@article{nishimoto_reconstructing_2011,
	title = {Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies},
	volume = {21},
	issn = {0960-9822},
	url = {https://www.sciencedirect.com/science/article/pii/S0960982211009377},
	doi = {10.1016/j.cub.2011.08.031},
	abstract = {Quantitative modeling of human brain activity can provide crucial insights about cortical representations [1, 2] and can form the basis for brain decoding devices [3, 4, 5]. Recent functional magnetic resonance imaging ({fMRI}) studies have modeled brain activity elicited by static visual patterns and have reconstructed these patterns from brain activity [6, 7, 8]. However, blood oxygen level-dependent ({BOLD}) signals measured via {fMRI} are very slow [9], so it has been difficult to model brain activity elicited by dynamic stimuli such as natural movies. Here we present a new motion-energy [10, 11] encoding model that largely overcomes this limitation. The model describes fast visual information and slow hemodynamics by separate components. We recorded {BOLD} signals in occipitotemporal visual cortex of human subjects who watched natural movies and fit the model separately to individual voxels. Visualization of the fit models reveals how early visual areas represent the information in movies. To demonstrate the power of our approach, we also constructed a Bayesian decoder [8] by combining estimated encoding models with a sampled natural movie prior. The decoder provides remarkable reconstructions of the viewed movies. These results demonstrate that dynamic brain activity measured under naturalistic conditions can be decoded using current {fMRI} technology.},
	pages = {1641--1646},
	number = {19},
	journaltitle = {Current Biology},
	shortjournal = {Current Biology},
	author = {Nishimoto, Shinji and Vu, An T. and Naselaris, Thomas and Benjamini, Yuval and Yu, Bin and Gallant, Jack L.},
	urldate = {2023-05-08},
	date = {2011-10-11},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/Users/syrkis/Zotero/storage/CSRWTXJY/Nishimoto et al. - 2011 - Reconstructing Visual Experiences from Brain Activ.pdf:application/pdf;ScienceDirect Snapshot:/Users/syrkis/Zotero/storage/X6FZSDTC/S0960982211009377.html:text/html},
}

@article{miyawaki_visual_2008,
	title = {Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders},
	volume = {60},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627308009586},
	doi = {10.1016/j.neuron.2008.11.004},
	abstract = {Perceptual experience consists of an enormous number of possible states. Previous {fMRI} studies have predicted a perceptual state by classifying brain activity into prespecified categories. Constraint-free visual image reconstruction is more challenging, as it is impractical to specify brain activity for all possible images. In this study, we reconstructed visual images by combining local image bases of multiple scales, whose contrasts were independently decoded from {fMRI} activity by automatically selecting relevant voxels and exploiting their correlated patterns. Binary-contrast, 10 × 10-patch images (2100 possible states) were accurately reconstructed without any image prior on a single trial or volume basis by measuring brain activity only for several hundred random images. Reconstruction was also used to identify the presented image among millions of candidates. The results suggest that our approach provides an effective means to read out complex perceptual states from brain activity while discovering information representation in multivoxel patterns.},
	pages = {915--929},
	number = {5},
	journaltitle = {Neuron},
	shortjournal = {Neuron},
	author = {Miyawaki, Yoichi and Uchida, Hajime and Yamashita, Okito and Sato, Masa-aki and Morito, Yusuke and Tanabe, Hiroki C. and Sadato, Norihiro and Kamitani, Yukiyasu},
	urldate = {2023-05-08},
	date = {2008-12-10},
	langid = {english},
	keywords = {{SYSNEURO}},
	file = {ScienceDirect Full Text PDF:/Users/syrkis/Zotero/storage/9ZXPJCX9/Miyawaki et al. - 2008 - Visual Image Reconstruction from Human Brain Activ.pdf:application/pdf;ScienceDirect Snapshot:/Users/syrkis/Zotero/storage/ZWRHDLCM/S0896627308009586.html:text/html},
}

@misc{wang_neuris_2022,
	title = {{NeuRIS}: Neural Reconstruction of Indoor Scenes Using Normal Priors},
	url = {http://arxiv.org/abs/2206.13597},
	shorttitle = {{NeuRIS}},
	abstract = {Reconstructing 3D indoor scenes from 2D images is an important task in many computer vision and graphics applications. A main challenge in this task is that large texture-less areas in typical indoor scenes make existing methods struggle to produce satisfactory reconstruction results. We propose a new method, named {NeuRIS}, for high quality reconstruction of indoor scenes. The key idea of {NeuRIS} is to integrate estimated normal of indoor scenes as a prior in a neural rendering framework for reconstructing large texture-less shapes and, importantly, to do this in an adaptive manner to also enable the reconstruction of irregular shapes with fine details. Specifically, we evaluate the faithfulness of the normal priors on-the-fly by checking the multi-view consistency of reconstruction during the optimization process. Only the normal priors accepted as faithful will be utilized for 3D reconstruction, which typically happens in the regions of smooth shapes possibly with weak texture. However, for those regions with small objects or thin structures, for which the normal priors are usually unreliable, we will only rely on visual features of the input images, since such regions typically contain relatively rich visual features (e.g., shade changes and boundary contours). Extensive experiments show that {NeuRIS} significantly outperforms the state-of-the-art methods in terms of reconstruction quality.},
	number = {{arXiv}:2206.13597},
	publisher = {{arXiv}},
	author = {Wang, Jiepeng and Wang, Peng and Long, Xiaoxiao and Theobalt, Christian and Komura, Taku and Liu, Lingjie and Wang, Wenping},
	urldate = {2023-05-08},
	date = {2022-10-16},
	eprinttype = {arxiv},
	eprint = {2206.13597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/syrkis/Zotero/storage/Y46FXZMQ/2206.html:text/html;Full Text PDF:/Users/syrkis/Zotero/storage/94EXYNG3/Wang et al. - 2022 - NeuRIS Neural Reconstruction of Indoor Scenes Usi.pdf:application/pdf},
}

@article{wein_graph_2021,
	title = {A graph neural network framework for causal inference in brain networks},
	volume = {11},
	rights = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-87411-8},
	doi = {10.1038/s41598-021-87411-8},
	abstract = {A central question in neuroscience is how self-organizing dynamic interactions in the brain emerge on their relatively static structural backbone. Due to the complexity of spatial and temporal dependencies between different brain areas, fully comprehending the interplay between structure and function is still challenging and an area of intense research. In this paper we present a graph neural network ({GNN}) framework, to describe functional interactions based on the structural anatomical layout. A {GNN} allows us to process graph-structured spatio-temporal signals, providing a possibility to combine structural information derived from diffusion tensor imaging ({DTI}) with temporal neural activity profiles, like that observed in functional magnetic resonance imaging ({fMRI}). Moreover, dynamic interactions between different brain regions discovered by this data-driven approach can provide a multi-modal measure of causal connectivity strength. We assess the proposed model’s accuracy by evaluating its capabilities to replicate empirically observed neural activation profiles, and compare the performance to those of a vector auto regression ({VAR}), like that typically used in Granger causality. We show that {GNNs} are able to capture long-term dependencies in data and also computationally scale up to the analysis of large-scale networks. Finally we confirm that features learned by a {GNN} can generalize across {MRI} scanner types and acquisition protocols, by demonstrating that the performance on small datasets can be improved by pre-training the {GNN} on data from an earlier study. We conclude that the proposed multi-modal {GNN} framework can provide a novel perspective on the structure-function relationship in the brain. Accordingly this approach appears to be promising for the characterization of the information flow in brain networks.},
	pages = {8061},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Wein, S. and Malloni, W. M. and Tomé, A. M. and Frank, S. M. and Henze, G.-I. and Wüst, S. and Greenlee, M. W. and Lang, E. W.},
	urldate = {2023-05-08},
	date = {2021-04-13},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational science, Dynamical systems, Network models},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/K7WLEYG4/Wein et al. - 2021 - A graph neural network framework for causal infere.pdf:application/pdf},
}

@incollection{gurbuz_deep_2020,
	title = {Deep Graph Normalizer: A Geometric Deep Learning Approach for Estimating Connectional Brain Templates},
	volume = {12267},
	url = {http://arxiv.org/abs/2012.14131},
	shorttitle = {Deep Graph Normalizer},
	abstract = {A connectional brain template ({CBT}) is a normalized graph-based representation of a population of brain networks also regarded as an average connectome. {CBTs} are powerful tools for creating representative maps of brain connectivity in typical and atypical populations. Particularly, estimating a well-centered and representative {CBT} for populations of multi-view brain networks ({MVBN}) is more challenging since these networks sit on complex manifolds and there is no easy way to fuse different heterogeneous network views. This problem remains unexplored with the exception of a few recent works rooted in the assumption that the relationship between connectomes are mostly linear. However, such an assumption fails to capture complex patterns and non-linear variation across individuals. Besides, existing methods are simply composed of sequential {MVBN} processing blocks without any feedback mechanism, leading to error accumulation. To address these issues, we propose Deep Graph Normalizer ({DGN}), the first geometric deep learning ({GDL}) architecture for normalizing a population of {MVBNs} by integrating them into a single connectional brain template. Our end-to-end {DGN} learns how to fuse multi-view brain networks while capturing non-linear patterns across subjects and preserving brain graph topological properties by capitalizing on graph convolutional neural networks. We also introduce a randomized weighted loss function which also acts as a regularizer to minimize the distance between the population of {MVBNs} and the estimated {CBT}, thereby enforcing its centeredness. We demonstrate that {DGN} significantly outperforms existing state-of-the-art methods on estimating {CBTs} on both small-scale and large-scale connectomic datasets in terms of both representativeness and discriminability (i.e., identifying distinctive connectivities fingerprinting each brain network population).},
	pages = {155--165},
	author = {Gurbuz, Mustafa Burak and Rekik, Islem},
	urldate = {2023-05-08},
	date = {2020},
	doi = {10.1007/978-3-030-59728-3_16},
	eprinttype = {arxiv},
	eprint = {2012.14131 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:/Users/syrkis/Zotero/storage/JVMTMVIE/2012.html:text/html;Full Text PDF:/Users/syrkis/Zotero/storage/XLVKCK23/Gurbuz and Rekik - 2020 - Deep Graph Normalizer A Geometric Deep Learning A.pdf:application/pdf},
}

@article{wang_dilated_2019,
	title = {Dilated 3D Convolutional Neural Networks for Brain {MRI} Data Classification},
	volume = {7},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2019.2941912},
	abstract = {Benefiting from the research of machine learning ({ML}) and deep learning({DL}), multivariate methods based on {ML} and {DL} have been the mainstream and successful analysis methods in Neural Engineering or Neuroimaging research, for example, assisting diagnosis based on brain Magnetic Resonance Imaging ({MRI}). However, many existing methods based on traditional {ML} methods cannot sufficiently extract discriminative features, especially feature patterns across long-distance brain areas, resulting in unsatisfactory classification performance. Designing an effective and robust classifier for different {MRI} images remains a challenge. In this paper, we introduced dilated 3D {CNN} method for classifying 3D {MRI} images combining {CNN} structure and dilated convolution with a small number of feature maps. We also presented a methodology framework based on dilated 3D {CNN} method, which can classify both single {MRI} images and image sequences. Our method and framework were evaluated on the structural {MRI} images of {ADHD}-200 dataset and {fMRI} images of a Schizophrenia dataset, demonstrating better performances than some other state-of-the-art methods.},
	pages = {134388--134398},
	journaltitle = {{IEEE} Access},
	author = {Wang, Zijian and Sun, Yaoru and Shen, Qianzi and Cao, Lei},
	date = {2019},
	note = {Conference Name: {IEEE} Access},
	keywords = {machine learning, Neuroimaging, Functional magnetic resonance imaging, Biomedical image processing, Convolution, Feature extraction, Machine learning algorithms, magnetic resonance imaging, Three-dimensional displays},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/ICWHJKQ9/Wang et al. - 2019 - Dilated 3D Convolutional Neural Networks for Brain.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/syrkis/Zotero/storage/9KSLIINN/08840843.html:text/html},
}

@article{meng_decoding_2022,
	title = {Decoding Visual {fMRI} Stimuli from Human Brain Based on Graph Convolutional Neural Network},
	volume = {12},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3425},
	url = {https://www.mdpi.com/2076-3425/12/10/1394},
	doi = {10.3390/brainsci12101394},
	abstract = {Brain decoding is to predict the external stimulus information from the collected brain response activities, and visual information is one of the most important sources of external stimulus information. Decoding functional magnetic resonance imaging ({fMRI}) based on visual stimulation is helpful in understanding the working mechanism of the brain visual function regions. Traditional brain decoding algorithms cannot accurately extract stimuli features from {fMRI}. To address these shortcomings, this paper proposed a brain decoding algorithm based on a graph convolution network ({GCN}). Firstly, 11 regions of interest ({ROI}) were selected according to the human brain visual function regions, which can avoid the noise interference of the non-visual regions of the human brain; then, a deep three-dimensional convolution neural network was specially designed to extract the features of these 11 regions; next, the {GCN} was used to extract the functional correlation features between the different human brain visual regions. Furthermore, to avoid the problem of gradient disappearance when there were too many layers of graph convolutional neural network, the residual connections were adopted in our algorithm, which helped to integrate different levels of features in order to improve the accuracy of the proposed {GCN}. The proposed algorithm was tested on the public dataset, and the recognition accuracy reached 98.67\%. Compared with the other state-of-the-art algorithms, the proposed algorithm performed the best.},
	pages = {1394},
	number = {10},
	journaltitle = {Brain Sciences},
	author = {Meng, Lu and Ge, Kang},
	urldate = {2023-05-09},
	date = {2022-10},
	langid = {english},
	note = {Number: 10
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {brain decoding, convolutional neural network, functional magnetic resonance image, graph convolution},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/JEUAYGEY/Meng and Ge - 2022 - Decoding Visual fMRI Stimuli from Human Brain Base.pdf:application/pdf},
}

@article{shen_end--end_2019,
	title = {End-to-End Deep Image Reconstruction From Human Brain Activity},
	volume = {13},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2019.00021},
	abstract = {Deep neural networks ({DNNs}) have recently been applied successfully to brain decoding and image reconstruction from functional magnetic resonance imaging ({fMRI}) activity. However, direct training of a {DNN} with {fMRI} data is often avoided because the size of available data is thought to be insufficient for training a complex network with numerous parameters. Instead, a pre-trained {DNN} usually serves as a proxy for hierarchical visual representations, and {fMRI} data are used to decode individual {DNN} features of a stimulus image using a simple linear model, which are then passed to a reconstruction module. Here, we directly trained a {DNN} model with {fMRI} data and the corresponding stimulus images to build an end-to-end reconstruction model. We accomplished this by training a generative adversarial network with an additional loss term that was defined in high-level feature space (feature loss) using up to 6,000 training data samples (natural images and {fMRI} responses). The above model was tested on independent datasets and directly reconstructed image using an {fMRI} pattern as the input. Reconstructions obtained from our proposed method resembled the test stimuli (natural and artificial images) and reconstruction accuracy increased as a function of training-data size. Ablation analyses indicated that the feature loss that we employed played a critical role in achieving accurate reconstruction. Our results show that the end-to-end model can learn a direct mapping between brain activity and perception.},
	journaltitle = {Frontiers in Computational Neuroscience},
	author = {Shen, Guohua and Dwivedi, Kshitij and Majima, Kei and Horikawa, Tomoyasu and Kamitani, Yukiyasu},
	urldate = {2023-05-09},
	date = {2019},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/ZDHII8FB/Shen et al. - 2019 - End-to-End Deep Image Reconstruction From Human Br.pdf:application/pdf},
}

@article{singer_spatiotemporal_nodate,
	title = {“The spatiotemporal neural dynamics of object recognition for natural images and line drawings”},
	author = {Singer, Johannes},
	langid = {english},
	file = {Singer - “The spatiotemporal neural dynamics of object reco.pdf:/Users/syrkis/Zotero/storage/R4W2HLW3/Singer - “The spatiotemporal neural dynamics of object reco.pdf:application/pdf},
}

@article{thomas_benchmarking_2023,
	title = {Benchmarking explanation methods for mental state decoding with deep learning models},
	volume = {273},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811923002550},
	doi = {10.1016/j.neuroimage.2023.120109},
	abstract = {Deep learning ({DL}) models find increasing application in mental state decoding, where researchers seek to understand the mapping between mental states (e.g., experiencing anger or joy) and brain activity by identifying those spatial and temporal features of brain activity that allow to accurately identify (i.e., decode) these states. Once a {DL} model has been trained to accurately decode a set of mental states, neuroimaging researchers often make use of methods from explainable artificial intelligence research to understand the model’s learned mappings between mental states and brain activity. Here, we benchmark prominent explanation methods in a mental state decoding analysis of multiple functional Magnetic Resonance Imaging ({fMRI}) datasets. Our findings demonstrate a gradient between two key characteristics of an explanation in mental state decoding, namely, its faithfulness and its alignment with other empirical evidence on the mapping between brain activity and decoded mental state: explanation methods with high explanation faithfulness, which capture the model’s decision process well, generally provide explanations that align less well with other empirical evidence than the explanations of methods with less faithfulness. Based on our findings, we provide guidance for neuroimaging researchers on how to choose an explanation method to gain insight into the mental state decoding decisions of {DL} models.},
	pages = {120109},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Thomas, Armin W. and Ré, Christopher and Poldrack, Russell A.},
	urldate = {2023-05-10},
	date = {2023-06-01},
	langid = {english},
	keywords = {Benchmark, Deep learning, Explainable {AI}, Mental state decoding, Neuroimaging},
	file = {ScienceDirect Full Text PDF:/Users/syrkis/Zotero/storage/NP3XHV8N/Thomas et al. - 2023 - Benchmarking explanation methods for mental state .pdf:application/pdf;ScienceDirect Snapshot:/Users/syrkis/Zotero/storage/BYUWBFU2/S1053811923002550.html:text/html},
}

@article{roth_natural_2022,
	title = {Natural scene sampling reveals reliable coarse-scale orientation tuning in human V1},
	volume = {13},
	rights = {2022 This is a U.S. Government work and not under copyright protection in the {US}; foreign copyright protection may apply},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-34134-7},
	doi = {10.1038/s41467-022-34134-7},
	abstract = {Orientation selectivity in primate visual cortex is organized into cortical columns. Since cortical columns are at a finer spatial scale than the sampling resolution of standard {BOLD} {fMRI} measurements, analysis approaches have been proposed to peer past these spatial resolution limitations. It was recently found that these methods are predominantly sensitive to stimulus vignetting - a form of selectivity arising from an interaction of the oriented stimulus with the aperture edge. Beyond vignetting, it is not clear whether orientation-selective neural responses are detectable in {BOLD} measurements. Here, we leverage a dataset of visual cortical responses measured using high-field 7T {fMRI}. Fitting these responses using image-computable models, we compensate for vignetting and nonetheless find reliable tuning for orientation. Results further reveal a coarse-scale map of orientation preference that may constitute the neural basis for known perceptual anisotropies. These findings settle a long-standing debate in human neuroscience, and provide insights into functional organization principles of visual cortex.},
	pages = {6469},
	number = {1},
	journaltitle = {Nature Communications},
	shortjournal = {Nat Commun},
	author = {Roth, Zvi N. and Kay, Kendrick and Merriam, Elisha P.},
	urldate = {2023-05-10},
	date = {2022-10-29},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Neural encoding, Striate cortex},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/9Q2MEPGH/Roth et al. - 2022 - Natural scene sampling reveals reliable coarse-sca.pdf:application/pdf},
}

@misc{gifford_algonauts_2023,
	title = {The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of Natural Scenes},
	url = {http://arxiv.org/abs/2301.03198},
	shorttitle = {The Algonauts Project 2023 Challenge},
	abstract = {The sciences of biological and artificial intelligence are ever more intertwined. Neural computational principles inspire new intelligent machines, which are in turn used to advance theoretical understanding of the brain. To promote further exchange of ideas and collaboration between biological and artificial intelligence researchers, we introduce the 2023 installment of the Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes (http://algonauts.csail.mit.edu). This installment prompts the fields of artificial and biological intelligence to come together towards building computational models of the visual brain using the largest and richest dataset of {fMRI} responses to visual scenes, the Natural Scenes Dataset ({NSD}). {NSD} provides high-quality {fMRI} responses to {\textasciitilde}73,000 different naturalistic colored scenes, making it the ideal candidate for data-driven model building approaches promoted by the 2023 challenge. The challenge is open to all and makes results directly comparable and transparent through a public leaderboard automatically updated after each submission, thus allowing for rapid model development. We believe that the 2023 installment will spark symbiotic collaborations between biological and artificial intelligence scientists, leading to a deeper understanding of the brain through cutting-edge computational models and to novel ways of engineering artificial intelligent agents through inductive biases from biological systems.},
	number = {{arXiv}:2301.03198},
	publisher = {{arXiv}},
	author = {Gifford, A. T. and Lahner, B. and Saba-Sadiya, S. and Vilas, M. G. and Lascelles, A. and Oliva, A. and Kay, K. and Roig, G. and Cichy, R. M.},
	urldate = {2023-05-10},
	date = {2023-01-10},
	eprinttype = {arxiv},
	eprint = {2301.03198 [cs, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	file = {arXiv.org Snapshot:/Users/syrkis/Zotero/storage/VNW58B9D/2301.html:text/html;Full Text PDF:/Users/syrkis/Zotero/storage/DC9DP6XB/Gifford et al. - 2023 - The Algonauts Project 2023 Challenge How the Huma.pdf:application/pdf},
}

@online{lin_mind_2022,
	title = {Mind Reader: Reconstructing complex images from brain activities},
	url = {https://arxiv.org/abs/2210.01769v1},
	shorttitle = {Mind Reader},
	abstract = {Understanding how the brain encodes external stimuli and how these stimuli can be decoded from the measured brain activities are long-standing and challenging questions in neuroscience. In this paper, we focus on reconstructing the complex image stimuli from {fMRI} (functional magnetic resonance imaging) signals. Unlike previous works that reconstruct images with single objects or simple shapes, our work aims to reconstruct image stimuli that are rich in semantics, closer to everyday scenes, and can reveal more perspectives. However, data scarcity of {fMRI} datasets is the main obstacle to applying state-of-the-art deep learning models to this problem. We find that incorporating an additional text modality is beneficial for the reconstruction problem compared to directly translating brain signals to images. Therefore, the modalities involved in our method are: (i) voxel-level {fMRI} signals, (ii) observed images that trigger the brain signals, and (iii) textual description of the images. To further address data scarcity, we leverage an aligned vision-language latent space pre-trained on massive datasets. Instead of training models from scratch to find a latent space shared by the three modalities, we encode {fMRI} signals into this pre-aligned latent space. Then, conditioned on embeddings in this space, we reconstruct images with a generative model. The reconstructed images from our pipeline balance both naturalness and fidelity: they are photo-realistic and capture the ground truth image contents well.},
	titleaddon = {{arXiv}.org},
	author = {Lin, Sikun and Sprague, Thomas and Singh, Ambuj K.},
	urldate = {2023-05-10},
	date = {2022-09-30},
	langid = {english},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/8UN45JR7/Lin et al. - 2022 - Mind Reader Reconstructing complex images from br.pdf:application/pdf},
}

@online{rakhimberdina_natural_2021,
	title = {Natural Image Reconstruction from {fMRI} using Deep Learning: A Survey},
	url = {https://arxiv.org/abs/2110.09006v2},
	shorttitle = {Natural Image Reconstruction from {fMRI} using Deep Learning},
	abstract = {With the advent of brain imaging techniques and machine learning tools, much effort has been devoted to building computational models to capture the encoding of visual information in the human brain. One of the most challenging brain decoding tasks is the accurate reconstruction of the perceived natural images from brain activities measured by functional magnetic resonance imaging ({fMRI}). In this work, we survey the most recent deep learning methods for natural image reconstruction from {fMRI}. We examine these methods in terms of architectural design, benchmark datasets, and evaluation metrics and present a fair performance evaluation across standardized evaluation metrics. Finally, we discuss the strengths and limitations of existing studies and present potential future directions.},
	titleaddon = {{arXiv}.org},
	author = {Rakhimberdina, Zarina and Jodelet, Quentin and Liu, Xin and Murata, Tsuyoshi},
	urldate = {2023-05-10},
	date = {2021-10-18},
	langid = {english},
	file = {Full Text:/Users/syrkis/Zotero/storage/Y3MUENIF/Rakhimberdina et al. - 2021 - Natural Image Reconstruction from fMRI using Deep .pdf:application/pdf;Full Text PDF:/Users/syrkis/Zotero/storage/UYTIZSCU/Rakhimberdina et al. - 2021 - Natural Image Reconstruction from fMRI using Deep .pdf:application/pdf},
}

@misc{lin_mind_2022-1,
	title = {Mind Reader: Reconstructing complex images from brain activities},
	url = {http://arxiv.org/abs/2210.01769},
	shorttitle = {Mind Reader},
	abstract = {Understanding how the brain encodes external stimuli and how these stimuli can be decoded from the measured brain activities are long-standing and challenging questions in neuroscience. In this paper, we focus on reconstructing the complex image stimuli from {fMRI} (functional magnetic resonance imaging) signals. Unlike previous works that reconstruct images with single objects or simple shapes, our work aims to reconstruct image stimuli that are rich in semantics, closer to everyday scenes, and can reveal more perspectives. However, data scarcity of {fMRI} datasets is the main obstacle to applying state-of-the-art deep learning models to this problem. We find that incorporating an additional text modality is beneficial for the reconstruction problem compared to directly translating brain signals to images. Therefore, the modalities involved in our method are: (i) voxel-level {fMRI} signals, (ii) observed images that trigger the brain signals, and (iii) textual description of the images. To further address data scarcity, we leverage an aligned vision-language latent space pre-trained on massive datasets. Instead of training models from scratch to find a latent space shared by the three modalities, we encode {fMRI} signals into this pre-aligned latent space. Then, conditioned on embeddings in this space, we reconstruct images with a generative model. The reconstructed images from our pipeline balance both naturalness and fidelity: they are photo-realistic and capture the ground truth image contents well.},
	number = {{arXiv}:2210.01769},
	publisher = {{arXiv}},
	author = {Lin, Sikun and Sprague, Thomas and Singh, Ambuj K.},
	urldate = {2023-05-10},
	date = {2022-09-30},
	eprinttype = {arxiv},
	eprint = {2210.01769 [cs, eess, q-bio]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Computer Science - Human-Computer Interaction, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv.org Snapshot:/Users/syrkis/Zotero/storage/JFU745YQ/2210.html:text/html;Full Text PDF:/Users/syrkis/Zotero/storage/6HRB4LCE/Lin et al. - 2022 - Mind Reader Reconstructing complex images from br.pdf:application/pdf},
}

@article{allen_massive_2022,
	title = {A massive 7T {fMRI} dataset to bridge cognitive neuroscience and artificial intelligence},
	volume = {25},
	rights = {2021 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-021-00962-x},
	doi = {10.1038/s41593-021-00962-x},
	abstract = {Extensive sampling of neural activity during rich cognitive phenomena is critical for robust understanding of brain function. Here we present the Natural Scenes Dataset ({NSD}), in which high-resolution functional magnetic resonance imaging responses to tens of thousands of richly annotated natural scenes were measured while participants performed a continuous recognition task. To optimize data quality, we developed and applied novel estimation and denoising techniques. Simple visual inspections of the {NSD} data reveal clear representational transformations along the ventral visual pathway. Further exemplifying the inferential power of the dataset, we used {NSD} to build and train deep neural network models that predict brain activity more accurately than state-of-the-art models from computer vision. {NSD} also includes substantial resting-state and diffusion data, enabling network neuroscience perspectives to constrain and enhance models of perception and memory. Given its unprecedented scale, quality and breadth, {NSD} opens new avenues of inquiry in cognitive neuroscience and artificial intelligence.},
	pages = {116--126},
	number = {1},
	journaltitle = {Nature Neuroscience},
	shortjournal = {Nat Neurosci},
	author = {Allen, Emily J. and St-Yves, Ghislain and Wu, Yihan and Breedlove, Jesse L. and Prince, Jacob S. and Dowdle, Logan T. and Nau, Matthias and Caron, Brad and Pestilli, Franco and Charest, Ian and Hutchinson, J. Benjamin and Naselaris, Thomas and Kay, Kendrick},
	urldate = {2023-05-11},
	date = {2022-01},
	langid = {english},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Perception, Cortex, Neural encoding, Object vision},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/ISLGZJJC/Allen et al. - 2022 - A massive 7T fMRI dataset to bridge cognitive neur.pdf:application/pdf},
}

@article{nishimoto_reconstructing_2011-1,
	title = {Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies},
	volume = {21},
	issn = {0960-9822},
	url = {https://www.cell.com/current-biology/abstract/S0960-9822(11)00937-7},
	doi = {10.1016/j.cub.2011.08.031},
	pages = {1641--1646},
	number = {19},
	journaltitle = {Current Biology},
	shortjournal = {Current Biology},
	author = {Nishimoto, Shinji and Vu, An T. and Naselaris, Thomas and Benjamini, Yuval and Yu, Bin and Gallant, Jack L.},
	urldate = {2023-05-11},
	date = {2011-10-11},
	pmid = {21945275},
	file = {Nishimoto et al. - 2011 - Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies.pdf:/Users/syrkis/Zotero/storage/EYJWGXBM/Nishimoto et al. - 2011 - Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies.pdf:application/pdf},
}

@article{rokem_fractional_2020,
	title = {Fractional ridge regression: a fast, interpretable reparameterization of ridge regression},
	volume = {9},
	issn = {2047-217X},
	url = {https://doi.org/10.1093/gigascience/giaa133},
	doi = {10.1093/gigascience/giaa133},
	shorttitle = {Fractional ridge regression},
	abstract = {Ridge regression is a regularization technique that penalizes the L2-norm of the coefficients in linear regression. One of the challenges of using ridge regression is the need to set a hyperparameter (α) that controls the amount of regularization. Cross-validation is typically used to select the best α from a set of candidates. However, efficient and appropriate selection of α can be challenging. This becomes prohibitive when large amounts of data are analyzed. Because the selected α depends on the scale of the data and correlations across predictors, it is also not straightforwardly interpretable.The present work addresses these challenges through a novel approach to ridge regression. We propose to reparameterize ridge regression in terms of the ratio γ between the L2-norms of the regularized and unregularized coefficients. We provide an algorithm that efficiently implements this approach, called fractional ridge regression, as well as open-source software implementations in Python and matlab (https://github.com/nrdg/fracridge). We show that the proposed method is fast and scalable for large-scale data problems. In brain imaging data, we demonstrate that this approach delivers results that are straightforward to interpret and compare across models and datasets.Fractional ridge regression has several benefits: the solutions obtained for different γ are guaranteed to vary, guarding against wasted calculations; and automatically span the relevant range of regularization, avoiding the need for arduous manual exploration. These properties make fractional ridge regression particularly suitable for analysis of large complex datasets.},
	pages = {giaa133},
	number = {12},
	journaltitle = {{GigaScience}},
	shortjournal = {{GigaScience}},
	author = {Rokem, Ariel and Kay, Kendrick},
	urldate = {2023-05-11},
	date = {2020-11-30},
	file = {Rokem and Kay - 2020 - Fractional ridge regression a fast, interpretable reparameterization of ridge regression.pdf:/Users/syrkis/Zotero/storage/ENRMRZNJ/Rokem and Kay - 2020 - Fractional ridge regression a fast, interpretable reparameterization of ridge regression.pdf:application/pdf},
}

@article{kumar_brainiak_2020,
	title = {{BrainIAK} tutorials: User-friendly learning materials for advanced {fMRI} analysis},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007549},
	doi = {10.1371/journal.pcbi.1007549},
	shorttitle = {{BrainIAK} tutorials},
	abstract = {Advanced brain imaging analysis methods, including multivariate pattern analysis ({MVPA}), functional connectivity, and functional alignment, have become powerful tools in cognitive neuroscience over the past decade. These tools are implemented in custom code and separate packages, often requiring different software and language proficiencies. Although usable by expert researchers, novice users face a steep learning curve. These difficulties stem from the use of new programming languages (e.g., Python), learning how to apply machine-learning methods to high-dimensional {fMRI} data, and minimal documentation and training materials. Furthermore, most standard {fMRI} analysis packages (e.g., {AFNI}, {FSL}, {SPM}) focus on preprocessing and univariate analyses, leaving a gap in how to integrate with advanced tools. To address these needs, we developed {BrainIAK} (brainiak.org), an open-source Python software package that seamlessly integrates several cutting-edge, computationally efficient techniques with other Python packages (e.g., Nilearn, Scikit-learn) for file handling, visualization, and machine learning. To disseminate these powerful tools, we developed user-friendly tutorials (in Jupyter format; https://brainiak.org/tutorials/) for learning {BrainIAK} and advanced {fMRI} analysis in Python more generally. These materials cover techniques including: {MVPA} (pattern classification and representational similarity analysis); parallelized searchlight analysis; background connectivity; full correlation matrix analysis; inter-subject correlation; inter-subject functional connectivity; shared response modeling; event segmentation using hidden Markov models; and real-time {fMRI}. For long-running jobs or large memory needs we provide detailed guidance on high-performance computing clusters. These notebooks were successfully tested at multiple sites, including as problem sets for courses at Yale and Princeton universities and at various workshops and hackathons. These materials are freely shared, with the hope that they become part of a pool of open-source software and educational materials for large-scale, reproducible {fMRI} analysis and accelerated discovery.},
	pages = {e1007549},
	number = {1},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Kumar, Manoj and Ellis, Cameron T. and Lu, Qihong and Zhang, Hejia and Capotă, Mihai and Willke, Theodore L. and Ramadge, Peter J. and Turk-Browne, Nicholas B. and Norman, Kenneth A.},
	urldate = {2023-05-12},
	date = {2020-01-15},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Cognitive neuroscience, Computer software, Functional magnetic resonance imaging, Human learning, Machine learning, Machine learning algorithms, Open source software, Programming languages},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/5NUMHXGZ/Kumar et al. - 2020 - BrainIAK tutorials User-friendly learning materia.pdf:application/pdf},
}

@misc{feilong_cortical_2023,
	title = {A cortical surface template for human neuroscience},
	rights = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), {CC} {BY} 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2023.03.21.533686v1},
	doi = {10.1101/2023.03.21.533686},
	abstract = {Neuroimaging data analysis relies on normalization to standard anatomical templates to resolve macroanatomical differences across brains. Existing human cortical surface templates sample locations unevenly because of distortions introduced by inflation of the folded cortex into a standard shape. Here we present the onavg template, which affords uniform sampling of the cortex. We created the onavg template based on openly-available high-quality structural scans of 1,031 brains—25 times more than existing cortical templates. We optimized the vertex locations based on cortical anatomy, achieving an even distribution. We observed consistently higher multivariate pattern classification accuracies and representational geometry inter-subject correlations based on onavg than on other templates, and onavg only needs 3⁄4 as much data to achieve the same performance compared to other templates. The optimized sampling also reduces {CPU} time across algorithms by 1.3\%–22.4\% due to less variation in the number of vertices in each searchlight.},
	publisher = {{bioRxiv}},
	author = {Feilong, Ma and Jiahui, Guo and Gobbini, M. Ida and Haxby, James V.},
	urldate = {2023-05-12},
	date = {2023-03-24},
	langid = {english},
	note = {Pages: 2023.03.21.533686
Section: New Results},
	file = {Full Text PDF:/Users/syrkis/Zotero/storage/2K27ZRRQ/Feilong et al. - 2023 - A cortical surface template for human neuroscience.pdf:application/pdf},
}

@article{zaretskaya_advantages_2018,
	title = {Advantages of cortical surface reconstruction using submillimeter 7 Tesla {MEMPRAGE}},
	volume = {165},
	issn = {1053-8119},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6383677/},
	doi = {10.1016/j.neuroimage.2017.09.060},
	abstract = {Recent advances in {MR} technology have enabled increased spatial resolution for routine functional and anatomical imaging, which has created demand for software tools that are able to process these data. The availability of high-resolution data also raises the question of whether higher resolution leads to substantial gains in accuracy of quantitative morphometric neuroimaging procedures, in particular the cortical surface reconstruction and cortical thickness estimation. In this study we adapted the {FreeSurfer} cortical surface reconstruction pipeline to process structural data at native submillimeter resolution. We then quantified the differences in surface placement between meshes generated from 0.75 mm isotropic resolution data acquired in 39 volunteers and the same data downsampled to the conventional 1 mm3 voxel size. We find that when processed at native resolution, cortex is estimated to be thinner in most areas, but thicker around the Cingulate and the Calcarine sulci as well as in the posterior bank of the Central sulcus. Thickness differences are driven by two kinds of effects. First, the gray–white surface is found closer to the white matter, especially in cortical areas with high myelin content, and thus low contrast, such as the Calcarine and the Central sulci, causing local increases in thickness estimates. Second, the gray–{CSF} surface is placed more interiorly, especially in the deep sulci, contributing to local decreases in thickness estimates. We suggest that both effects are due to reduced partial volume effects at higher spatial resolution. Submillimeter voxel sizes can therefore provide improved accuracy for measuring cortical thickness.},
	pages = {11--26},
	journaltitle = {{NeuroImage}},
	shortjournal = {Neuroimage},
	author = {Zaretskaya, Natalia and Fischl, Bruce and Reuter, Martin and Renvall, Ville and Polimeni, Jonathan R.},
	urldate = {2023-05-12},
	date = {2018-01-15},
	pmid = {28970143},
	pmcid = {PMC6383677},
	file = {PubMed Central Full Text PDF:/Users/syrkis/Zotero/storage/R6YJ5RNJ/Zaretskaya et al. - 2018 - Advantages of cortical surface reconstruction usin.pdf:application/pdf},
}

@article{tong_decoding_2012,
	title = {Decoding Patterns of Human Brain Activity},
	volume = {63},
	issn = {0066-4308},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7869795/},
	doi = {10.1146/annurev-psych-120710-100412},
	abstract = {Considerable information about mental states can be decoded from non-invasive measures of human brain activity. Analyses of brain activity patterns can reveal what a person is seeing, perceiving, attending to, or remembering. Moreover, multidimensional models can be used to investigate how the brain encodes complex visual scenes or abstract semantic information. Such feats of “brain reading” or “mind reading”, though impressive, raise important conceptual, methodological, and ethical issues. What does successful decoding reveal about the cognitive functions performed by a brain region? How should brain signals be spatially selected and mathematically combined, to ensure that decoding reflects inherent computations of the brain rather than those performed by the decoder? We will highlight recent advances and describe how multivoxel pattern analysis ({MVPA}) can provide a window into mind-brain relationships with unprecedented specificity, when carefully applied. However, as brain-reading technology advances, issues of neuroethics and mental privacy will be important to consider.},
	pages = {483--509},
	journaltitle = {Annual review of psychology},
	shortjournal = {Annu Rev Psychol},
	author = {Tong, Frank and Pratte, Michael S.},
	urldate = {2023-05-13},
	date = {2012},
	pmid = {21943172},
	pmcid = {PMC7869795},
	file = {PubMed Central Full Text PDF:/Users/syrkis/Zotero/storage/YW24RG3C/Tong and Pratte - 2012 - Decoding Patterns of Human Brain Activity.pdf:application/pdf},
}

@article{naselaris_bayesian_2009,
	title = {Bayesian Reconstruction of Natural Images from Human Brain Activity},
	volume = {63},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627309006850},
	doi = {10.1016/j.neuron.2009.09.006},
	abstract = {Recent studies have used {fMRI} signals from early visual areas to reconstruct simple geometric patterns. Here, we demonstrate a new Bayesian decoder that uses {fMRI} signals from early and anterior visual areas to reconstruct complex natural images. Our decoder combines three elements: a structural encoding model that characterizes responses in early visual areas, a semantic encoding model that characterizes responses in anterior visual areas, and prior information about the structure and semantic content of natural images. By combining all these elements, the decoder produces reconstructions that accurately reflect both the spatial structure and semantic category of the objects contained in the observed natural image. Our results show that prior information has a substantial effect on the quality of natural image reconstructions. We also demonstrate that much of the variance in the responses of anterior visual areas to complex natural images is explained by the semantic category of the image alone.},
	pages = {902--915},
	number = {6},
	journaltitle = {Neuron},
	shortjournal = {Neuron},
	author = {Naselaris, Thomas and Prenger, Ryan J. and Kay, Kendrick N. and Oliver, Michael and Gallant, Jack L.},
	urldate = {2023-05-13},
	date = {2009-09-24},
	langid = {english},
	keywords = {{SYSNEURO}},
	file = {ScienceDirect Full Text PDF:/Users/syrkis/Zotero/storage/AEBNM5JU/Naselaris et al. - 2009 - Bayesian Reconstruction of Natural Images from Hum.pdf:application/pdf;ScienceDirect Snapshot:/Users/syrkis/Zotero/storage/TT5NCVFD/S0896627309006850.html:text/html},
}

@article{haxby_distributed_2001,
	title = {Distributed and overlapping representations of faces and objects in ventral temporal cortex},
	volume = {293},
	issn = {0036-8075},
	doi = {10.1126/science.1063736},
	abstract = {The functional architecture of the object vision pathway in the human brain was investigated using functional magnetic resonance imaging to measure patterns of response in ventral temporal cortex while subjects viewed faces, cats, five categories of man-made objects, and nonsense pictures. A distinct pattern of response was found for each stimulus category. The distinctiveness of the response to a given category was not due simply to the regions that responded maximally to that category, because the category being viewed also could be identified on the basis of the pattern of response when those regions were excluded from the analysis. Patterns of response that discriminated among all categories were found even within cortical regions that responded maximally to only one category. These results indicate that the representations of faces and objects in ventral temporal cortex are widely distributed and overlapping.},
	pages = {2425--2430},
	number = {5539},
	journaltitle = {Science (New York, N.Y.)},
	shortjournal = {Science},
	author = {Haxby, J. V. and Gobbini, M. I. and Furey, M. L. and Ishai, A. and Schouten, J. L. and Pietrini, P.},
	date = {2001-09-28},
	pmid = {11577229},
	keywords = {Brain Mapping, Face, Female, Form Perception, Humans, Magnetic Resonance Imaging, Male, Pattern Recognition, Visual, Recognition, Psychology, Temporal Lobe, Visual Pathways},
	file = {Accepted Version:/Users/syrkis/Zotero/storage/CLWPS83F/Haxby et al. - 2001 - Distributed and overlapping representations of fac.pdf:application/pdf},
}
