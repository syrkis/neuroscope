{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroscope\n",
    "\n",
    "Jupyter workspace for the neuroscope project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import vmap, jit, lax, random, grad, value_and_grad\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from jax import config\n",
    "import jraph\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, pickle\n",
    "\n",
    "import syrkis\n",
    "from src.data import load_subjects, make_kfolds\n",
    "from src.fmri import get_bold_with_coords_and_faces as get_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "rng = random.PRNGKey(0)\n",
    "cfg = syrkis.train.load_config()['neuroscope']\n",
    "opt = optax.adamw(learning_rate=cfg['lr'])\n",
    "config.update(\"jax_platform_name\", \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = load_subjects(['subj05', 'subj07'], cfg['image_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples(sample, subject, n_subjects):\n",
    "    lhs, rhs, imgs = sample\n",
    "    fmri = []\n",
    "    for idx, lh, rh in tqdm(zip(range(n_subjects), lhs, rhs)):\n",
    "        lh_graph  = make_graph(*get_mesh(lh, subject, 'lh'))\n",
    "        rh_graph  = make_graph(*get_mesh(rh, subject, 'rh'))\n",
    "        graph     = combine_hems(lh_graph, rh_graph)\n",
    "        fmri.append(graph)\n",
    "    # fmri = jnp.array(fmri)\n",
    "    return fmri, imgs\n",
    "\n",
    "def combine_hems(lh_graph, rh_graph):\n",
    "    # Concatenate node feature\n",
    "    # number of 0s we need to add to have nodes be power of 2\n",
    "    n_node = lh_graph.n_node + rh_graph.n_node\n",
    "    padding_size = (2 ** jnp.ceil(jnp.log2(n_node)) - n_node).astype(int)[0]\n",
    "    padding = jnp.zeros((padding_size, 1))\n",
    "    nodes = jnp.concatenate([lh_graph.nodes, rh_graph.nodes, padding], axis=0)\n",
    "\n",
    "    # Adjust senders and receivers indices for right hemisphere\n",
    "    rh_offset = lh_graph.n_node\n",
    "    rh_senders = rh_graph.senders + rh_offset\n",
    "    rh_receivers = rh_graph.receivers + rh_offset\n",
    "\n",
    "    senders = jnp.concatenate([lh_graph.senders, rh_senders], axis=0)\n",
    "    receivers = jnp.concatenate([lh_graph.receivers, rh_receivers], axis=0)\n",
    "\n",
    "    n_node += + padding_size\n",
    "    n_edge  = lh_graph.n_edge + rh_graph.n_edge\n",
    "    \n",
    "    return jraph.GraphsTuple(n_node=n_node, n_edge=n_edge, edges=None, globals=None,\n",
    "                             nodes=nodes, senders=senders, receivers=receivers)\n",
    "\n",
    "\n",
    "@jit\n",
    "def make_graph(coords, features, faces):\n",
    "    senders   = jnp.concatenate([faces[:, 0], faces[:, 1], faces[:, 2]], axis=0)\n",
    "    receivers = jnp.concatenate([faces[:, 1], faces[:, 2], faces[:, 0]], axis=0)\n",
    "    n_node = jnp.array([features.shape[0]])\n",
    "    n_edge = jnp.array([senders.shape[0]])\n",
    "\n",
    "    graph = jraph.GraphsTuple(n_node=n_node, n_edge=n_edge, edges=None, globals=None,\n",
    "                              nodes=features[:, None], senders=senders, receivers=receivers)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_node_fn(node_features, params):\n",
    "    weights, biases = params\n",
    "    return jnp.dot(node_features, weights) + biases\n",
    "\n",
    "def apply_graph_convolution(graph, params):\n",
    "    # Define the graph convolution layer\n",
    "    gcn_layer = jraph.GraphConvolution(\n",
    "        update_node_fn=partial(update_node_fn, params=params),\n",
    "        aggregate_nodes_fn=jraph.segment_sum,\n",
    "        add_self_edges=True,\n",
    "        symmetric_normalization=True,\n",
    "    )\n",
    "\n",
    "    # Apply the graph convolution layer to the graph\n",
    "    return gcn_layer(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pool_fn(graph, pool_size):\n",
    "    # Reshape and pool node features\n",
    "    num_nodes, num_features = graph.nodes.shape\n",
    "    pooled_features = graph.nodes.reshape(-1, pool_size, num_features)\n",
    "    pooled_features = jnp.mean(pooled_features, axis=1)\n",
    "\n",
    "    # Update edges for the pooled graph\n",
    "    # Create a mapping from old node indices to new pooled node indices\n",
    "    node_mapping = np.repeat(np.arange(len(pooled_features)), pool_size)[:num_nodes]\n",
    "\n",
    "    # Update senders and receivers based on the node mapping\n",
    "    pooled_senders = node_mapping[graph.senders]\n",
    "    pooled_receivers = node_mapping[graph.receivers]\n",
    "\n",
    "    # Filter out self-loops created by pooling\n",
    "    edge_mask = pooled_senders != pooled_receivers\n",
    "    pooled_senders = pooled_senders[edge_mask]\n",
    "    pooled_receivers = pooled_receivers[edge_mask]\n",
    "\n",
    "    # Update the graph with pooled nodes and edges\n",
    "    pooled_graph = graph._replace(nodes=pooled_features, senders=pooled_senders, receivers=pooled_receivers)\n",
    "    return pooled_graph\n",
    "\n",
    "# Example usage\n",
    "# Assume 'graph' is your input jraph.GraphsTuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "def latent_side_fn(cfg):\n",
    "    return cfg['image_size'] // cfg['stride'] ** cfg['conv_layers']\n",
    "\n",
    "def latent_dim_fn(cfg):\n",
    "    # should return the size of the loatente dim depending on initial image size, stride, and number of layers, and channels\n",
    "    channels = cfg['chan_start']\n",
    "    # calulate latent channels\n",
    "    latent_channels = int(channels * (cfg['conv_branch'] ** (cfg['conv_layers'] - 1)))\n",
    "    # calculate latent side\n",
    "    latent_side = latent_side_fn(cfg)\n",
    "    # calculate latent dim\n",
    "    latent_dim = latent_channels * latent_side ** 2\n",
    "    return latent_dim\n",
    "        \n",
    "latent_dim = latent_dim_fn(cfg)\n",
    "latent_side = latent_side_fn(cfg)\n",
    "print(latent_side)\n",
    "print(latent_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_norm(x, gamma, beta, eps=1e-5):\n",
    "    if not cfg['batch_norm']:\n",
    "        return x\n",
    "    # x: batch x height x width x channels\n",
    "    axis = tuple(range(len(x.shape) - 1))\n",
    "    mean = jnp.mean(x, axis=axis, keepdims=True)\n",
    "    var = jnp.var(x, axis=axis, keepdims=True)\n",
    "    x = (x - mean) / jnp.sqrt(var + eps)\n",
    "    x = gamma * x + beta\n",
    "    return x\n",
    "\n",
    "def init_batch_norm(shape):\n",
    "    shape = [1 for _ in range(len(shape) - 1)] + [shape[-1]]\n",
    "    shape = tuple(shape)\n",
    "    gamma = jnp.ones(shape)\n",
    "    beta = jnp.zeros(shape)\n",
    "    return gamma, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear_layer(rng, in_dim, out_dim, tensor_dim):\n",
    "    # tensor dim is for having fmri embedding in same array, but seperate layers.\n",
    "    rng, key = jax.random.split(rng, 2)\n",
    "    w_shape = (in_dim, out_dim)\n",
    "    b_shape = (out_dim,)\n",
    "    w = syrkis.train.glorot_init(key, w_shape)\n",
    "    if tensor_dim > 0:\n",
    "        w = w.reshape((-1, out_dim, tensor_dim))\n",
    "    b = jnp.zeros(b_shape)\n",
    "    gamma, beta = init_batch_norm(b_shape)\n",
    "    return w, b, gamma, beta\n",
    "\n",
    "def linear(params, x):\n",
    "    for idx, (w, b, gamma, beta) in enumerate(params):\n",
    "        x = x @ w + b\n",
    "        x = jax.nn.gelu(x) if idx != len(params) - 1 else x\n",
    "        x = batch_norm(x, gamma, beta) if idx != len(params) - 1 else x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants for common parameters\n",
    "DIMENSION_NUMBERS = (\"NHWC\", \"HWIO\", \"NHWC\")\n",
    "\n",
    "\n",
    "@jit\n",
    "def upscale_nearest_neighbor(x, scale_factor=cfg['stride']):\n",
    "    # Assuming x has shape (batch, height, width, channels)\n",
    "    b, h, w, c = x.shape\n",
    "    x = x.reshape(b, h, 1, w, 1, c)\n",
    "    x = lax.tie_in(x, jnp.broadcast_to(x, (b, h, scale_factor, w, scale_factor, c)))\n",
    "    return x.reshape(b, h * scale_factor, w * scale_factor, c)\n",
    "\n",
    "\n",
    "@jit\n",
    "def deconv2d(x, w):\n",
    "    x_upscaled = upscale_nearest_neighbor(x)\n",
    "    return lax.conv_transpose(\n",
    "        x_upscaled, w, \n",
    "        strides=(1, 1), \n",
    "        padding='SAME',\n",
    "        dimension_numbers=DIMENSION_NUMBERS) \n",
    "\n",
    "\n",
    "def conv_fn(fn):\n",
    "    def apply_fn(params, x):\n",
    "        for i, (w, b) in enumerate(params):\n",
    "            x = fn(x, w, b)\n",
    "            # x = batch_norm(x, gamma, beta) if i != len(params) - 1 else x\n",
    "            # x = jax.nn.tanh(x) if i != len(params) - 1 else x\n",
    "            x = jax.nn.gelu(x) if i != len(params) - 1 else x\n",
    "        return x\n",
    "    return apply_fn\n",
    "\n",
    "\n",
    "def manual_batch_graphs(graph_list):\n",
    "    # Initialize lists to hold the concatenated components\n",
    "    all_nodes = []\n",
    "    all_senders = []\n",
    "    all_receivers = []\n",
    "    offset = 0\n",
    "\n",
    "    for graph in graph_list:\n",
    "        all_nodes.append(graph.nodes)\n",
    "        all_senders.append(graph.senders + offset)\n",
    "        all_receivers.append(graph.receivers + offset)\n",
    "        offset += graph.nodes.shape[0]\n",
    "\n",
    "    # Concatenate all components\n",
    "    batched_nodes = jnp.concatenate(all_nodes, axis=0)\n",
    "    batched_senders = jnp.concatenate(all_senders, axis=0)\n",
    "    batched_receivers = jnp.concatenate(all_receivers, axis=0)\n",
    "\n",
    "    # Create and return the combined GraphsTuple\n",
    "    return jraph.GraphsTuple(\n",
    "        n_node=jnp.array(batched_nodes.shape[0]),\n",
    "        n_edge=jnp.array(batched_senders.shape[0]),\n",
    "        nodes=batched_nodes,\n",
    "        senders=batched_senders,\n",
    "        receivers=batched_receivers,\n",
    "        edges=None,  # or concatenate edges if your graph has them\n",
    "        globals=None  # or concatenate globals if your graph has them\n",
    "    )\n",
    "\n",
    "\n",
    "deconv = conv_fn(lambda x, w, b: deconv2d(x, w) + b)\n",
    "\n",
    "\n",
    "def init_conv_params(rng, in_chan, out_chan, cfg, deconv=False):\n",
    "    if deconv:\n",
    "        in_chan, out_chan = out_chan, in_chan\n",
    "    rng, key = jax.random.split(rng, 2)\n",
    "    w_shape = (cfg['kernel_size'], cfg['kernel_size'], in_chan, out_chan)\n",
    "    w = syrkis.train.glorot_init(key, w_shape)\n",
    "    b = jnp.zeros((out_chan,))\n",
    "    gamma, beta = init_batch_norm(b.shape)\n",
    "    return w, b # , gamma, beta\n",
    "\n",
    "\n",
    "def init_conv_layers(rng, cfg, deconv=False):\n",
    "    rngs = jax.random.split(rng, cfg['conv_layers'])\n",
    "    params = []\n",
    "    for idx, rng in enumerate(rngs):\n",
    "        in_chan = cfg['in_chans'] if idx == 0 else cfg['chan_start'] * (cfg['conv_branch'] ** (idx - 1))\n",
    "        out_chan = cfg['chan_start'] * (cfg['conv_branch'] ** idx)\n",
    "        params.append(init_conv_params(rng, in_chan, out_chan, cfg, deconv))\n",
    "    return params[::-1] if deconv else params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, rate, rng):\n",
    "    rate = 1.0 - rate\n",
    "    keep = random.bernoulli(rng, rate, x.shape)\n",
    "    return jnp.where(keep, x / rate, 0)\n",
    "\n",
    "\n",
    "def matmul_slice(A, B_slice):\n",
    "    return jnp.dot(A, B_slice)\n",
    "batched_matmul = vmap(matmul_slice, in_axes=(None, 2))\n",
    "\n",
    "\n",
    "@jit\n",
    "def decode_fn(params, z, rng=None):\n",
    "    z = deconv(params['deconv'], z)\n",
    "    z = jax.nn.sigmoid(z)\n",
    "    return z\n",
    "\n",
    "\n",
    "def apply_fn(params, fmri):\n",
    "    z = fmri\n",
    "\n",
    "    # Apply graph convolution layers\n",
    "    for i, p in enumerate(params['gcn']):\n",
    "        z = apply_graph_convolution(z, p)\n",
    "        z = pool_fn(z, 4)\n",
    "    z = z.nodes.flatten()\n",
    "\n",
    "    # Apply dense layer\n",
    "    for i, p in enumerate(params['fcs']):\n",
    "        z = jnp.dot(z, p[0]) + p[1]\n",
    "        z = jax.nn.relu(z)\n",
    "\n",
    "    # Apply image deconv layers to make image\n",
    "    z = z.reshape(1, 2, 2, -1)\n",
    "    z = deconv(params['cnn'], z)\n",
    "    z = jax.nn.sigmoid(z)\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "# This function returns the total loss and its components (recon and KL losses).\n",
    "def loss_fn(params, fmri, img):\n",
    "    img_hat = apply_fn(params, fmri)\n",
    "    recon_loss = jnp.mean((img - img_hat) ** 2)\n",
    "    return recon_loss\n",
    "\n",
    "def update_fn(params, fmri, img, subj, opt_state):\n",
    "    # Get the loss, aux data (recon_loss, kl_loss), and gradients\n",
    "    loss, grads = value_and_grad(loss_fn)(params, fmri, img, subj)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "def init_fn(rng, cfg, scale=1e-2):\n",
    "    gcn = []\n",
    "    c_in = 1\n",
    "    for i in range(cfg['gcn_layers']):\n",
    "        rng, key = random.split(rng)\n",
    "        c_out = 2 ** i \n",
    "        gcn.append((random.normal(key, (c_in, c_out)) * scale, jnp.zeros((c_out,))))\n",
    "        c_in = c_out\n",
    "    \n",
    "    rng, key = random.split(rng)\n",
    "    cnn = init_conv_layers(key, cfg, deconv=True)\n",
    "    rng, key = random.split(rng)\n",
    "    fcs = [\n",
    "        (random.normal(key, (1024, cfg['latent_dim'])) * scale, jnp.zeros((cfg['latent_dim'],))),\n",
    "        (random.normal(key, (cfg['latent_dim'], 2048)) * scale, jnp.zeros((2048,))),\n",
    "    ]\n",
    "    return {'gcn': gcn, 'fcs': fcs, 'cnn': cnn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1963083"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = syrkis.train.load_config()['neuroscope']\n",
    "rng = jax.random.PRNGKey(0)\n",
    "params = init_fn(rng, cfg)\n",
    "n_params = syrkis.train.n_params(params)\n",
    "opt_state = opt.init(params)\n",
    "syrkis.train.n_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [00:10,  9.45it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All input arrays must have the same shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fmri, imgs \u001b[38;5;241m=\u001b[39m make_samples(subjects[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubj05\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubj05\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m fmri \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m paraply_fn \u001b[38;5;241m=\u001b[39m jit(vmap(apply_fn, in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m2\u001b[39m)):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:2037\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   2036\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m-> 2037\u001b[0m     out \u001b[38;5;241m=\u001b[39m stack(\u001b[43m[\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43melt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   2038\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2039\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:2037\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   2036\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m-> 2037\u001b[0m     out \u001b[38;5;241m=\u001b[39m stack([\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43melt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m elt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mobject\u001b[39m])\n\u001b[1;32m   2038\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2039\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:2070\u001b[0m, in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2069\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mcanonicalize_dtype(dtype, allow_opaque_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 2070\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:2037\u001b[0m, in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin)\u001b[0m\n\u001b[1;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mobject\u001b[39m, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   2036\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m-> 2037\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43melt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2038\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2039\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([], dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:1778\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays:\n\u001b[1;32m   1777\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape(a) \u001b[38;5;241m!=\u001b[39m shape0:\n\u001b[0;32m-> 1778\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll input arrays must have the same shape.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1779\u001b[0m   new_arrays\u001b[38;5;241m.\u001b[39mappend(expand_dims(a, axis))\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concatenate(new_arrays, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: All input arrays must have the same shape."
     ]
    }
   ],
   "source": [
    "fmri, imgs = make_samples(subjects['subj05'], 'subj05', 100)\n",
    "fmri = jnp.array(fmri)\n",
    "paraply_fn = jit(vmap(apply_fn, in_axes=(None, 0)))\n",
    "for i in tqdm(range(0, 100, 2)):\n",
    "    x, y = fmri[i:i+2], imgs[i:i+2]\n",
    "    y_hat = paraply_fn(params, x)\n",
    "    loss = loss_fn(params, x, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "UNKNOWN: /var/folders/g5/r0c49hqx3f95cpg6vnztk5s80000gn/T/ipykernel_88449/498112577.py:1:16: error: failed to legalize operation 'mhlo.pad'\n/var/folders/g5/r0c49hqx3f95cpg6vnztk5s80000gn/T/ipykernel_88449/498112577.py:1:16: note: called from\n/var/folders/g5/r0c49hqx3f95cpg6vnztk5s80000gn/T/ipykernel_88449/498112577.py:1:16: note: see current operation: %89 = \"mhlo.pad\"(%88, %1) {edge_padding_high = dense<0> : tensor<1xi64>, edge_padding_low = dense<0> : tensor<1xi64>, interior_padding = dense<1> : tensor<1xi64>} : (tensor<2xsi32>, tensor<si32>) -> tensor<3xsi32>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m batched_graph \u001b[38;5;241m=\u001b[39m \u001b[43mjraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmri\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jraph/_src/utils.py:477\u001b[0m, in \u001b[0;36mbatch\u001b[0;34m(graphs)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch\u001b[39m(graphs: Sequence[gn_graph\u001b[38;5;241m.\u001b[39mGraphsTuple]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m gn_graph\u001b[38;5;241m.\u001b[39mGraphsTuple:\n\u001b[1;32m    425\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a batched graph given a list of graphs.\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \n\u001b[1;32m    427\u001b[0m \u001b[38;5;124;03m  This method will concatenate the ``nodes``, ``edges`` and ``globals``,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m      graph.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjnp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jraph/_src/utils.py:489\u001b[0m, in \u001b[0;36m_batch\u001b[0;34m(graphs, np_)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns batched graph given a list of graphs and a numpy-like module.\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# Calculates offsets for sender and receiver arrays, caused by concatenating\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# the nodes arrays.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m offsets \u001b[38;5;241m=\u001b[39m \u001b[43mnp_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcumsum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_node\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraphs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_map_concat\u001b[39m(nests):\n\u001b[1;32m    493\u001b[0m   concat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs: np_\u001b[38;5;241m.\u001b[39mconcatenate(args)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/numpy/reductions.py:651\u001b[0m, in \u001b[0;36m_make_cumulative_reduction.<locals>.cumulative_reduction\u001b[0;34m(a, axis, dtype, out)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;129m@_wraps\u001b[39m(np_reduction, skip_params\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcumulative_reduction\u001b[39m(a: ArrayLike, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    650\u001b[0m                          dtype: DTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, out: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m--> 651\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_cumulative_reduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ensure_optional_axes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/dispatch.py:465\u001b[0m, in \u001b[0;36mbackend_compile\u001b[0;34m(backend, module, options, host_callbacks)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcompile(built_c, compile_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    461\u001b[0m                          host_callbacks\u001b[38;5;241m=\u001b[39mhost_callbacks)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Some backends don't have `host_callbacks` option yet\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# TODO(sharadmv): remove this fallback when all backends allow `compile`\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# to take in `host_callbacks`\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuilt_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: UNKNOWN: /var/folders/g5/r0c49hqx3f95cpg6vnztk5s80000gn/T/ipykernel_88449/498112577.py:1:16: error: failed to legalize operation 'mhlo.pad'\n/var/folders/g5/r0c49hqx3f95cpg6vnztk5s80000gn/T/ipykernel_88449/498112577.py:1:16: note: called from\n/var/folders/g5/r0c49hqx3f95cpg6vnztk5s80000gn/T/ipykernel_88449/498112577.py:1:16: note: see current operation: %89 = \"mhlo.pad\"(%88, %1) {edge_padding_high = dense<0> : tensor<1xi64>, edge_padding_low = dense<0> : tensor<1xi64>, interior_padding = dense<1> : tensor<1xi64>} : (tensor<2xsi32>, tensor<si32>) -> tensor<3xsi32>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batched_graph = jraph.batch(fmri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
