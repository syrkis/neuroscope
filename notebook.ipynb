{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroscope\n",
    "\n",
    "Jupyter workspace for the neuroscope project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import vmap, jit, lax, random, grad, value_and_grad\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from jax import config\n",
    "import jraph\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, pickle\n",
    "\n",
    "import syrkis\n",
    "from src.data import load_subjects, make_kfolds\n",
    "from src.fmri import get_bold_with_coords_and_faces as get_mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 09:44:02.928089: W pjrt_plugin/src/mps_client.cc:534] WARNING: JAX Apple GPU support is experimental and not all JAX functionality is correctly supported!\n"
     ]
    }
   ],
   "source": [
    "# GLOBALS\n",
    "rng = random.PRNGKey(0)\n",
    "cfg = syrkis.train.load_config()['neuroscope']\n",
    "opt = optax.adamw(learning_rate=cfg['lr'])\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = load_subjects(['subj05', 'subj07'], cfg['image_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_samples(rng, sample, subject, n_subjects=64):\n",
    "    lhs, rhs, imgs = sample\n",
    "    # shuffle\n",
    "    idxs = random.shuffle(rng, jnp.arange(len(lhs)))\n",
    "    lhs, rhs, imgs = lhs[idxs], rhs[idxs], imgs[idxs]\n",
    "    fmri = []\n",
    "    for idx, lh, rh in tqdm(zip(range(n_subjects), lhs, rhs)):\n",
    "        lh_graph  = make_graph(*get_mesh(lh, subject, 'lh'))\n",
    "        rh_graph  = make_graph(*get_mesh(rh, subject, 'rh'))\n",
    "        graph     = combine_hems(lh_graph, rh_graph)\n",
    "        fmri.append(graph)\n",
    "    # fmri = jnp.array(fmri)\n",
    "    return fmri, imgs\n",
    "\n",
    "def combine_hems(lh_graph, rh_graph):\n",
    "    # Concatenate node feature\n",
    "    # number of 0s we need to add to have nodes be power of 2\n",
    "    n_node = lh_graph.n_node + rh_graph.n_node\n",
    "    padding_size = (2 ** jnp.ceil(jnp.log2(n_node)) - n_node).astype(int)[0]\n",
    "    padding = jnp.zeros((padding_size))\n",
    "    nodes = jnp.concatenate([lh_graph.nodes, rh_graph.nodes, padding], axis=0)[:, None]\n",
    "\n",
    "    # Adjust senders and receivers indices for right hemisphere\n",
    "    rh_offset = lh_graph.n_node\n",
    "    rh_senders = rh_graph.senders + rh_offset\n",
    "    rh_receivers = rh_graph.receivers + rh_offset\n",
    "\n",
    "    senders = jnp.concatenate([lh_graph.senders, rh_senders], axis=0)\n",
    "    receivers = jnp.concatenate([lh_graph.receivers, rh_receivers], axis=0)\n",
    "\n",
    "    n_node += + padding_size\n",
    "    n_edge  = lh_graph.n_edge + rh_graph.n_edge\n",
    "\n",
    "    # make graph nodes power of 2 by padding with 0s\n",
    "    return jraph.GraphsTuple(n_node=n_node, n_edge=n_edge, edges=None, globals=None,\n",
    "                             nodes=nodes, senders=senders, receivers=receivers)\n",
    "\n",
    "\n",
    "@jit\n",
    "def make_graph(coords, features, faces):\n",
    "    senders   = jnp.concatenate([faces[:, 0], faces[:, 1], faces[:, 2]], axis=0)\n",
    "    receivers = jnp.concatenate([faces[:, 1], faces[:, 2], faces[:, 0]], axis=0)\n",
    "    n_node = jnp.array([features.shape[0]])\n",
    "    n_edge = jnp.array([senders.shape[0]])\n",
    "    graph = jraph.GraphsTuple(n_node=n_node, n_edge=n_edge, edges=None, globals=None,\n",
    "                              nodes=features,senders=senders, receivers=receivers)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_node_fn(node_features, params):\n",
    "    weights, biases = params\n",
    "    return jnp.dot(node_features, weights) + biases\n",
    "\n",
    "def apply_graph_convolution(graph, params):\n",
    "    # Define the graph convolution layer\n",
    "    gcn_layer = jraph.GraphConvolution(\n",
    "        update_node_fn=partial(update_node_fn, params=params),\n",
    "        aggregate_nodes_fn=jraph.segment_sum,\n",
    "        add_self_edges=True,\n",
    "        symmetric_normalization=True,\n",
    "    )\n",
    "\n",
    "    # Apply the graph convolution layer to the graph\n",
    "    return gcn_layer(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def manual_batch_graphs(graph_list):\n",
    "    # Initialize lists to hold the concatenated components\n",
    "    all_nodes = []\n",
    "    all_senders = []\n",
    "    all_receivers = []\n",
    "    offset = 0\n",
    "\n",
    "    for graph in graph_list:\n",
    "        all_nodes.append(graph.nodes)\n",
    "        all_senders.append(graph.senders + offset)\n",
    "        all_receivers.append(graph.receivers + offset)\n",
    "        offset += graph.nodes.shape[0]\n",
    "\n",
    "    # Concatenate all components\n",
    "    batched_nodes = jnp.concatenate(all_nodes, axis=0)\n",
    "    batched_senders = jnp.concatenate(all_senders, axis=0)\n",
    "    batched_receivers = jnp.concatenate(all_receivers, axis=0)\n",
    "\n",
    "    print(batched_nodes.shape, batched_senders.shape, batched_receivers.shape)\n",
    "\n",
    "    # Create and return the combined GraphsTuple\n",
    "    return jraph.GraphsTuple(\n",
    "        n_node=jnp.array(batched_nodes.shape[0]),\n",
    "        n_edge=jnp.array(batched_senders.shape[0]),\n",
    "        nodes=batched_nodes,\n",
    "        senders=batched_senders,\n",
    "        receivers=batched_receivers,\n",
    "        edges=None,  # or concatenate edges if your graph has them\n",
    "        globals=None  # or concatenate globals if your graph has them\n",
    "    )\n",
    "\n",
    "def get_batches(fmri, imgs):\n",
    "    n_batches = len(fmri) // batch_size\n",
    "    batches = [manual_batch_graphs(fmri[i*batch_size:(i+1)*batch_size]) for i in range(n_batches)]\n",
    "    while True:\n",
    "        for batch in batches:\n",
    "            yield batch, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [00:07,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048576, 1) (3723360,) (3723360,)\n",
      "(1048576, 1) (3723360,) (3723360,)\n",
      "(1048576, 1) (3723360,) (3723360,)\n",
      "(1048576, 1) (3723360,) (3723360,)\n",
      "Pooling level 0, Graph shapes: (1048576, 1) (3723360,) (3723360,)\n",
      "Mask shape: (3723360,)\n",
      "Pooling level 1, Graph shapes: (524288, 1) (3723360,) (3723360,)\n",
      "Mask shape: (3723360,)\n",
      "Pooling level 2, Graph shapes: (131072, 1) (3010672,) (3010672,)\n",
      "Mask shape: (3010672,)\n",
      "Pooling level 3, Graph shapes: (16384, 1) (2692064,) (2692064,)\n",
      "Mask shape: (2692064,)\n",
      "Pooling level 4, Graph shapes: (1024, 1) (2639840,) (2639840,)\n",
      "Mask shape: (2639840,)\n",
      "Pooling level 5, Graph shapes: (32, 1) (1017152,) (1017152,)\n",
      "Mask shape: (1017152,)\n",
      "Original graph shapes: (1048576, 1) (3723360,) (3723360,)\n",
      "Pooling level 0, pool_size: 1\n",
      "Mask shape: (3723360,)\n",
      "Pooling level 0 passed.\n",
      "Pooling level 1, pool_size: 2\n",
      "Mask shape: (3723360,)\n",
      "Pooling level 1 passed.\n",
      "Pooling level 2, pool_size: 4\n",
      "Mask shape: (3010672,)\n",
      "Pooling level 2 passed.\n",
      "Pooling level 3, pool_size: 8\n",
      "Mask shape: (2692064,)\n",
      "Pooling level 3 passed.\n",
      "Pooling level 4, pool_size: 16\n",
      "Mask shape: (2639840,)\n",
      "Pooling level 4 passed.\n",
      "Pooling level 5, pool_size: 32\n",
      "Mask shape: (1017152,)\n",
      "Pooling level 5 passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphsTuple(nodes=Array([[ 0.28777757],\n",
       "       [ 0.05049862],\n",
       "       [-0.17920095],\n",
       "       [-0.051829  ],\n",
       "       [ 0.3068578 ],\n",
       "       [ 0.06397736],\n",
       "       [-0.15679964],\n",
       "       [-0.01175532],\n",
       "       [ 0.06080233],\n",
       "       [ 0.00094873],\n",
       "       [ 0.2437399 ],\n",
       "       [ 0.0499064 ],\n",
       "       [ 0.63817453],\n",
       "       [ 0.11681537],\n",
       "       [ 0.17500529],\n",
       "       [ 0.05185405],\n",
       "       [ 0.01888733],\n",
       "       [ 0.01546848],\n",
       "       [ 0.22708529],\n",
       "       [ 0.05428521],\n",
       "       [ 0.17166245],\n",
       "       [ 0.03111537],\n",
       "       [ 0.48542744],\n",
       "       [ 0.11151299],\n",
       "       [ 0.11951147],\n",
       "       [ 0.01147569],\n",
       "       [-0.253561  ],\n",
       "       [-0.05866559],\n",
       "       [-0.00262577],\n",
       "       [-0.00081914],\n",
       "       [-0.17068523],\n",
       "       [-0.05305072]], dtype=float32), edges=None, receivers=Array([ 0,  0,  0, ..., 31, 31, 31], dtype=int32), senders=Array([ 0,  0,  0, ..., 30, 30, 30], dtype=int32), globals=None, n_node=Array(1048576, dtype=int32, weak_type=True), n_edge=Array(3723360, dtype=int32, weak_type=True))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def pool_fn(graph, edge_mask, pool_size=2):\n",
    "    num_nodes, num_features = graph.nodes.shape\n",
    "\n",
    "    # Ensure the number of nodes is divisible by pool_size\n",
    "    if num_nodes % pool_size != 0:\n",
    "        raise ValueError(f\"Number of nodes ({num_nodes}) must be divisible by pool_size ({pool_size}).\")\n",
    "\n",
    "    # Pooling the nodes\n",
    "    pooled_features = graph.nodes.reshape(-1, pool_size, num_features)\n",
    "    pooled_features = jnp.mean(pooled_features, axis=1)\n",
    "\n",
    "    # Calculate new indices for senders and receivers\n",
    "    node_mapping = jnp.repeat(jnp.arange(num_nodes // pool_size), pool_size)\n",
    "    new_senders = node_mapping[graph.senders]\n",
    "    new_receivers = node_mapping[graph.receivers]\n",
    "\n",
    "    # Use the precomputed edge_mask to filter edges\n",
    "    new_senders = new_senders[edge_mask]\n",
    "    new_receivers = new_receivers[edge_mask]\n",
    "\n",
    "    # Update the graph with pooled nodes and edges\n",
    "    pooled_graph = graph._replace(nodes=pooled_features, senders=new_senders, receivers=new_receivers)\n",
    "    return pooled_graph\n",
    "\n",
    "\n",
    "def generate_valid_edge_mask(graph, pool_size):\n",
    "    num_nodes = graph.nodes.shape[0] // pool_size\n",
    "    node_mapping = jnp.repeat(jnp.arange(num_nodes), pool_size)\n",
    "\n",
    "    # Calculate new indices for senders and receivers\n",
    "    pooled_senders = node_mapping[graph.senders]\n",
    "    pooled_receivers = node_mapping[graph.receivers]\n",
    "\n",
    "    # Create a mask for valid edges (non-self-loops)\n",
    "    valid_edge_mask = pooled_senders != pooled_receivers\n",
    "    return valid_edge_mask\n",
    "\n",
    "\n",
    "def generate_valid_edge_masks(graph, pool_depth):\n",
    "    lst = []\n",
    "    for i in range(pool_depth):\n",
    "        pool_size = 2 ** i\n",
    "        if i > 0:  # Pool the graph for levels > 0\n",
    "            graph = pool_fn(graph, lst[i-1], pool_size)\n",
    "\n",
    "        mask = generate_valid_edge_mask(graph, pool_size)\n",
    "        lst.append(mask)\n",
    "        print(f'Pooling level {i}, Graph shapes:', graph.nodes.shape, graph.senders.shape, graph.receivers.shape)\n",
    "        print(f'Mask shape:', mask.shape)\n",
    "\n",
    "    return lst\n",
    "\n",
    "\n",
    "def test_pooling_function(graph, pool_depth):\n",
    "    print(\"Original graph shapes:\", graph.nodes.shape, graph.senders.shape, graph.receivers.shape)\n",
    "\n",
    "    for i in range(pool_depth):\n",
    "        pool_size = 2 ** i\n",
    "        if i > 0:  # Pool the graph for levels > 0\n",
    "            graph = pool_fn(graph, lst[i-1], pool_size)\n",
    "\n",
    "        mask = generate_valid_edge_mask(graph, pool_size)\n",
    "        print(f\"Pooling level {i}, pool_size: {pool_size}\")\n",
    "        print(\"Mask shape:\", mask.shape)\n",
    "\n",
    "        # Check if the mask and the number of edges align\n",
    "        if graph.senders.shape[0] != mask.shape[0]:\n",
    "            print(f\"Error at pooling level {i}: Edge count does not match mask size.\")\n",
    "        else:\n",
    "            print(f\"Pooling level {i} passed.\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "\n",
    "# Example usage\n",
    "fmri, imgs = make_samples(rng, subjects['subj05'], 'subj05')\n",
    "batches = get_batches(fmri, imgs)\n",
    "graph = next(batches)[0]\n",
    "lst = generate_valid_edge_masks(graph, 6)\n",
    "test_pooling_function(graph, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_side_fn(cfg):\n",
    "    return cfg['image_size'] // cfg['stride'] ** cfg['conv_layers']\n",
    "\n",
    "def latent_dim_fn(cfg):\n",
    "    # should return the size of the loatente dim depending on initial image size, stride, and number of layers, and channels\n",
    "    channels = cfg['chan_start']\n",
    "    # calulate latent channels\n",
    "    latent_channels = int(channels * (cfg['conv_branch'] ** (cfg['conv_layers'] - 1)))\n",
    "    # calculate latent side\n",
    "    latent_side = latent_side_fn(cfg)\n",
    "    # calculate latent dim\n",
    "    latent_dim = latent_channels * latent_side ** 2\n",
    "    return latent_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_norm(x, gamma, beta, eps=1e-5):\n",
    "    if not cfg['batch_norm']:\n",
    "        return x\n",
    "    # x: batch x height x width x channels\n",
    "    axis = tuple(range(len(x.shape) - 1))\n",
    "    mean = jnp.mean(x, axis=axis, keepdims=True)\n",
    "    var = jnp.var(x, axis=axis, keepdims=True)\n",
    "    x = (x - mean) / jnp.sqrt(var + eps)\n",
    "    x = gamma * x + beta\n",
    "    return x\n",
    "\n",
    "def init_batch_norm(shape):\n",
    "    shape = [1 for _ in range(len(shape) - 1)] + [shape[-1]]\n",
    "    shape = tuple(shape)\n",
    "    gamma = jnp.ones(shape)\n",
    "    beta = jnp.zeros(shape)\n",
    "    return gamma, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_linear_layer(rng, in_dim, out_dim, tensor_dim):\n",
    "    # tensor dim is for having fmri embedding in same array, but seperate layers.\n",
    "    rng, key = jax.random.split(rng, 2)\n",
    "    w_shape = (in_dim, out_dim)\n",
    "    b_shape = (out_dim,)\n",
    "    w = syrkis.train.glorot_init(key, w_shape)\n",
    "    if tensor_dim > 0:\n",
    "        w = w.reshape((-1, out_dim, tensor_dim))\n",
    "    b = jnp.zeros(b_shape)\n",
    "    gamma, beta = init_batch_norm(b_shape)\n",
    "    return w, b, gamma, beta\n",
    "\n",
    "def linear(params, x):\n",
    "    for idx, (w, b, gamma, beta) in enumerate(params):\n",
    "        x = x @ w + b\n",
    "        x = jax.nn.gelu(x) if idx != len(params) - 1 else x\n",
    "        x = batch_norm(x, gamma, beta) if idx != len(params) - 1 else x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants for common parameters\n",
    "DIMENSION_NUMBERS = (\"NHWC\", \"HWIO\", \"NHWC\")\n",
    "\n",
    "\n",
    "@jit\n",
    "def upscale_nearest_neighbor(x, scale_factor=cfg['stride']):\n",
    "    # Assuming x has shape (batch, height, width, channels)\n",
    "    b, h, w, c = x.shape\n",
    "    x = x.reshape(b, h, 1, w, 1, c)\n",
    "    x = lax.tie_in(x, jnp.broadcast_to(x, (b, h, scale_factor, w, scale_factor, c)))\n",
    "    return x.reshape(b, h * scale_factor, w * scale_factor, c)\n",
    "\n",
    "\n",
    "@jit\n",
    "def deconv2d(x, w):\n",
    "    x_upscaled = upscale_nearest_neighbor(x)\n",
    "    return lax.conv_transpose(\n",
    "        x_upscaled, w, \n",
    "        strides=(1, 1), \n",
    "        padding='SAME',\n",
    "        dimension_numbers=DIMENSION_NUMBERS) \n",
    "\n",
    "\n",
    "def conv_fn(fn):\n",
    "    def apply_fn(params, x):\n",
    "        for i, (w, b) in enumerate(params):\n",
    "            x = fn(x, w, b)\n",
    "            # x = batch_norm(x, gamma, beta) if i != len(params) - 1 else x\n",
    "            # x = jax.nn.tanh(x) if i != len(params) - 1 else x\n",
    "            x = jax.nn.gelu(x) if i != len(params) - 1 else x\n",
    "        return x\n",
    "    return apply_fn\n",
    "\n",
    "\n",
    "deconv = conv_fn(lambda x, w, b: deconv2d(x, w) + b)\n",
    "\n",
    "\n",
    "def init_conv_params(rng, in_chan, out_chan, cfg, deconv=False):\n",
    "    if deconv:\n",
    "        in_chan, out_chan = out_chan, in_chan\n",
    "    rng, key = jax.random.split(rng, 2)\n",
    "    w_shape = (cfg['kernel_size'], cfg['kernel_size'], in_chan, out_chan)\n",
    "    w = syrkis.train.glorot_init(key, w_shape)\n",
    "    b = jnp.zeros((out_chan,))\n",
    "    gamma, beta = init_batch_norm(b.shape)\n",
    "    return w, b # , gamma, beta\n",
    "\n",
    "\n",
    "def init_conv_layers(rng, cfg, deconv=False):\n",
    "    rngs = jax.random.split(rng, cfg['conv_layers'])\n",
    "    params = []\n",
    "    for idx, rng in enumerate(rngs):\n",
    "        in_chan = cfg['in_chans'] if idx == 0 else cfg['chan_start'] * (cfg['conv_branch'] ** (idx - 1))\n",
    "        out_chan = cfg['chan_start'] * (cfg['conv_branch'] ** idx)\n",
    "        params.append(init_conv_params(rng, in_chan, out_chan, cfg, deconv))\n",
    "    return params[::-1] if deconv else params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, rate, rng):\n",
    "    rate = 1.0 - rate\n",
    "    keep = random.bernoulli(rng, rate, x.shape)\n",
    "    return jnp.where(keep, x / rate, 0)\n",
    "\n",
    "\n",
    "def matmul_slice(A, B_slice):\n",
    "    return jnp.dot(A, B_slice)\n",
    "batched_matmul = vmap(matmul_slice, in_axes=(None, 2))\n",
    "\n",
    "\n",
    "@jit\n",
    "def decode_fn(params, z, rng=None):\n",
    "    z = deconv(params['deconv'], z)\n",
    "    z = jax.nn.sigmoid(z)\n",
    "    return z\n",
    "\n",
    "\n",
    "def apply_fn(params, fmri):\n",
    "    z = fmri\n",
    "    # Apply graph convolution layers\n",
    "    for i, p in enumerate(params['gcn']):\n",
    "        print(z.nodes.shape, z.senders.shape, z.receivers.shape)\n",
    "        z = apply_graph_convolution(z, p)\n",
    "        z = pool_fn(z, lst[i])\n",
    "    z = z.nodes.reshape(batch_size, -1)\n",
    "\n",
    "    # Apply dense layer\n",
    "    for i, p in enumerate(params['fcs']):\n",
    "        z = jnp.dot(z, p[0]) + p[1]\n",
    "        z = jax.nn.relu(z)\n",
    "\n",
    "    # Apply image deconv layers to make image\n",
    "    z = z.reshape(batch_size, 2, 2, -1)\n",
    "    z = deconv(params['cnn'], z)\n",
    "    z = jax.nn.sigmoid(z)\n",
    "    return z\n",
    "\n",
    "\n",
    "# This function returns the total loss and its components (recon and KL losses).\n",
    "def loss_fn(params, fmri, img):\n",
    "    img_hat = apply_fn(params, fmri)\n",
    "    recon_loss = jnp.mean((img - img_hat) ** 2)\n",
    "    return recon_loss\n",
    "\n",
    "@jit\n",
    "def update_fn(params, fmri, img, opt_state):\n",
    "    # Get the loss, aux data (recon_loss, kl_loss), and gradients\n",
    "    loss, grads = value_and_grad(loss_fn)(params, fmri, img)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "\n",
    "def init_fn(rng, cfg, scale=1e-2):\n",
    "    gcn = []\n",
    "    c_in = 1\n",
    "    for i in range(cfg['gcn_layers']):\n",
    "        rng, key = random.split(rng)\n",
    "        c_out = 2 ** i \n",
    "        gcn.append((random.normal(key, (c_in, c_out)) * scale, jnp.zeros((c_out,))))\n",
    "        c_in = c_out\n",
    "    \n",
    "    rng, key = random.split(rng)\n",
    "    cnn = init_conv_layers(key, cfg, deconv=True)\n",
    "    rng, key = random.split(rng)\n",
    "    fcs = [\n",
    "        (random.normal(key, (1024, cfg['latent_dim'])) * scale, jnp.zeros((cfg['latent_dim'],))),\n",
    "        (random.normal(key, (cfg['latent_dim'], 2048)) * scale, jnp.zeros((2048,))),\n",
    "    ]\n",
    "    return {'gcn': gcn, 'fcs': fcs, 'cnn': cnn}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1048576, 1) (3723360,) (3723360,)\n",
      "(1048576, 1) (3723360,) (3723360,)\n",
      "(1048576, 1) (3723360,) (3723360,)\n",
      "(1048576, 1) (3723360,) (3723360,)\n",
      "\n",
      "(1048576, 1) (3723360,) (3723360,)\n",
      "(524288, 1) (3723360,) (3723360,)\n",
      "(262144, 2) (3010672,) (3010672,)\n",
      "(131072, 4) (2692064,) (2692064,)\n",
      "(65536, 8) (2639840,) (2639840,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Incompatible shapes for dot: got (16, 32768) and (1024, 128).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m syrkis\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mn_params(params)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 30\u001b[0m, in \u001b[0;36mapply_fn\u001b[0;34m(params, fmri)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Apply dense layer\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfcs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 30\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m p[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     31\u001b[0m     z \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(z)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Apply image deconv layers to make image\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:3049\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(a, b, precision)\u001b[0m\n\u001b[1;32m   3047\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mmul(a, b)\n\u001b[1;32m   3048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(a_ndim, b_ndim) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 3049\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m b_ndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3052\u001b[0m   contract_dims \u001b[38;5;241m=\u001b[39m ((a_ndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,), (\u001b[38;5;241m0\u001b[39m,))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/lax/lax.py:698\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(lhs, rhs, precision, preferred_element_type)\u001b[0m\n\u001b[1;32m    694\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dot_general(lhs, rhs, (((lhs\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,), (\u001b[38;5;241m0\u001b[39m,)), ((), ())),\n\u001b[1;32m    695\u001b[0m                      precision\u001b[38;5;241m=\u001b[39mprecision,\n\u001b[1;32m    696\u001b[0m                      preferred_element_type\u001b[38;5;241m=\u001b[39mpreferred_element_type)\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 698\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible shapes for dot: got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    699\u001b[0m       lhs\u001b[38;5;241m.\u001b[39mshape, rhs\u001b[38;5;241m.\u001b[39mshape))\n",
      "\u001b[0;31mTypeError\u001b[0m: Incompatible shapes for dot: got (16, 32768) and (1024, 128)."
     ]
    }
   ],
   "source": [
    "# fmri, imgs = make_samples(rng, subjects['subj05'], 'subj05', 64)\n",
    "cfg        = syrkis.train.load_config()['neuroscope']\n",
    "rng        = jax.random.PRNGKey(0)\n",
    "params     = init_fn(rng, cfg)\n",
    "n_params   = syrkis.train.n_params(params)\n",
    "opt_state  = opt.init(params)\n",
    "batches    = get_batches(fmri, imgs)\n",
    "graph      = next(batches)[0]\n",
    "syrkis.train.n_params(params)\n",
    "print()\n",
    "apply_fn(params, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ConcretizationTypeError",
     "evalue": "Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThe error occurred while tracing the function apply_fn at /var/folders/g5/r0c49hqx3f95cpg6vnztk5s80000gn/T/ipykernel_88449/2396676233.py:19 for jit. This concrete value was not available in Python because it depends on the values of the arguments fmri.receivers and fmri.senders.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConcretizationTypeError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[239], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (pbar \u001b[38;5;241m:=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m))):\n\u001b[1;32m      4\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(batches)\n\u001b[0;32m----> 5\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" loss = loss_fn(params, x, y)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m    params, opt_state, loss = update_fn(params, x, y, opt_state)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m    pbar.set_description(f'loss: {loss:.4f}') \"\"\"\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[237], line 25\u001b[0m, in \u001b[0;36mapply_fn\u001b[0;34m(params, fmri)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgcn\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     24\u001b[0m     z \u001b[38;5;241m=\u001b[39m apply_graph_convolution(z, p)\n\u001b[0;32m---> 25\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mpool_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m z \u001b[38;5;241m=\u001b[39m z\u001b[38;5;241m.\u001b[39mnodes\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Apply dense layer\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[232], line 23\u001b[0m, in \u001b[0;36mpool_fn\u001b[0;34m(graph, pool_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m edge_mask \u001b[38;5;241m=\u001b[39m pooled_senders \u001b[38;5;241m!=\u001b[39m pooled_receivers\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Use jnp.where to gather valid edge indices\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m valid_edge_indices \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_mask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m pooled_senders \u001b[38;5;241m=\u001b[39m pooled_senders[valid_edge_indices]\n\u001b[1;32m     25\u001b[0m pooled_receivers \u001b[38;5;241m=\u001b[39m pooled_receivers[valid_edge_indices]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:1091\u001b[0m, in \u001b[0;36mwhere\u001b[0;34m(condition, x, y, size, fill_value)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1090\u001b[0m   util\u001b[38;5;241m.\u001b[39mcheck_arraylike(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m, condition)\n\u001b[0;32m-> 1091\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1093\u001b[0m   util\u001b[38;5;241m.\u001b[39mcheck_arraylike(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere\u001b[39m\u001b[38;5;124m\"\u001b[39m, condition, x, y)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:1358\u001b[0m, in \u001b[0;36mnonzero\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1356\u001b[0m   size \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m core\u001b[38;5;241m.\u001b[39mis_special_dim_size(size):\n\u001b[0;32m-> 1358\u001b[0m   size \u001b[38;5;241m=\u001b[39m \u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcrete_or_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe size argument of jnp.nonzero must be statically specified \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto use jnp.nonzero within JAX transformations.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1362\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(zeros(size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m dim \u001b[38;5;129;01min\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/site-packages/jax/_src/core.py:1356\u001b[0m, in \u001b[0;36mconcrete_or_error\u001b[0;34m(force, val, context)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m force(val\u001b[38;5;241m.\u001b[39maval\u001b[38;5;241m.\u001b[39mval)\n\u001b[1;32m   1355\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConcretizationTypeError(val, context)\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1358\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m force(val)\n",
      "\u001b[0;31mConcretizationTypeError\u001b[0m: Abstract tracer value encountered where concrete value is expected: Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>\nThe size argument of jnp.nonzero must be statically specified to use jnp.nonzero within JAX transformations.\nThe error occurred while tracing the function apply_fn at /var/folders/g5/r0c49hqx3f95cpg6vnztk5s80000gn/T/ipykernel_88449/2396676233.py:19 for jit. This concrete value was not available in Python because it depends on the values of the arguments fmri.receivers and fmri.senders.\n\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.ConcretizationTypeError"
     ]
    }
   ],
   "source": [
    "for i in (pbar := tqdm(range(10))):\n",
    "    x, y = next(batches)\n",
    "    y_hat = apply_fn(params, x)\n",
    "    \"\"\" loss = loss_fn(params, x, y)\n",
    "    params, opt_state, loss = update_fn(params, x, y, opt_state)\n",
    "    pbar.set_description(f'loss: {loss:.4f}') \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1048576, 1), (3723360,), (3723360,))"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.nodes.shape, graph.senders.shape, graph.receivers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
