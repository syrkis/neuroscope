{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syrkis\n",
    "import jax\n",
    "from jax import vmap, jit, lax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from src.data import load_subject, make_kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS\n",
    "cfg = {\n",
    "    'image_size' :   28,\n",
    "    'embed_dim'  :   10,\n",
    "    'batch_size' :   60,\n",
    "    'kernel_size':    3,\n",
    "    'channels'   :    1,\n",
    "    'stride'     :    1,\n",
    "    'layers'     :    2,\n",
    "    'lr'         :   1e-3,\n",
    "    'epochs'     :  100,\n",
    "    'scale'      :   1e-2,\n",
    "    'beta'       :   1.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = load_subject('subj07', image_size=cfg['image_size'])\n",
    "kfolds = make_kfolds(subject, cfg)\n",
    "loader, eval_loader = next(kfolds)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tfds.load('mnist', split='train', shuffle_files=True)\n",
    "# make it jax\n",
    "mnist = tfds.as_numpy(mnist)\n",
    "mnist = jnp.array([x['image'] for x in mnist]) / 255.\n",
    "mnist = mnist.reshape(-1, cfg['batch_size'], 28, 28, 1)\n",
    "mnist.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def batch_norm(x, gamma, beta, eps=1e-5):\n",
    "    # x: batch x height x width x channels\n",
    "    mean = jnp.mean(x, axis=(0, 1, 2), keepdims=True)\n",
    "    var = jnp.var(x, axis=(0, 1, 2), keepdims=True)\n",
    "    x = (x - mean) / jnp.sqrt(var + eps)\n",
    "    x = gamma * x + beta\n",
    "    return x\n",
    "\n",
    "def init_batch_norm(channels):\n",
    "    gamma = jnp.ones((1, 1, 1, channels))\n",
    "    beta = jnp.zeros((1, 1, 1, channels))\n",
    "    return gamma, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global constants for common parameters\n",
    "DIMENSION_NUMBERS = (\"NHWC\", \"HWIO\", \"NHWC\")\n",
    "\n",
    "@jit\n",
    "def conv2d(x, w):\n",
    "    return jax.lax.conv_general_dilated(\n",
    "        x, w, \n",
    "        window_strides=(cfg['stride'], cfg['stride']),\n",
    "        padding='SAME',\n",
    "        dimension_numbers=DIMENSION_NUMBERS)\n",
    "\n",
    "@jit\n",
    "def upscale_nearest_neighbor(x, scale_factor=cfg['stride']):\n",
    "    # Assuming x has shape (batch, height, width, channels)\n",
    "    b, h, w, c = x.shape\n",
    "    x = x.reshape(b, h, 1, w, 1, c)\n",
    "    x = lax.tie_in(x, jnp.broadcast_to(x, (b, h, scale_factor, w, scale_factor, c)))\n",
    "    return x.reshape(b, h * scale_factor, w * scale_factor, c)\n",
    "\n",
    "@jit\n",
    "def deconv2d(x, w):\n",
    "    x_upscaled = upscale_nearest_neighbor(x)\n",
    "    return lax.conv_transpose(\n",
    "        x_upscaled, w, \n",
    "        strides=(1, 1), \n",
    "        padding='SAME',\n",
    "        dimension_numbers=DIMENSION_NUMBERS) \n",
    "\n",
    "def conv_fn(fn):\n",
    "    def apply_fn(params, x):\n",
    "        for w, b, gamma, beta in params[:-1]:\n",
    "            x = fn(x, w, b)\n",
    "            x = batch_norm(x, gamma, beta)\n",
    "            x = jax.nn.gelu(x)\n",
    "        w, b, gamma, beta = params[-1]\n",
    "        x = fn(x, w, b)\n",
    "        return x\n",
    "    return apply_fn\n",
    "\n",
    "conv   = conv_fn(lambda x, w, b: conv2d(x, w) + b)\n",
    "deconv = conv_fn(lambda x, w, b: deconv2d(x, w) + b)\n",
    "\n",
    "def init_conv_params(rng, channels, kernel_size, scale, deconv=False):\n",
    "    rng, key1, key2 = jax.random.split(rng, 3)\n",
    "    out_channels = channels if deconv else channels * 2\n",
    "    in_channels  = channels if not deconv else channels * 2\n",
    "    w_shape = (kernel_size, kernel_size, in_channels, out_channels)\n",
    "    b_shape = (out_channels,)\n",
    "    w = scale * jax.random.normal(key1, w_shape)\n",
    "    b = scale * jax.random.normal(key2, b_shape)\n",
    "    gamma, beta = init_batch_norm(out_channels)\n",
    "    return w, b, gamma, beta\n",
    "\n",
    "def init_conv_layers(rng, channels, kernel_size, layers, scale, deconv=False):\n",
    "    rngs = jax.random.split(rng, layers)\n",
    "    params = []\n",
    "    for idx, rng in enumerate(rngs):\n",
    "        params.append(init_conv_params(rng, channels * 2 ** idx, kernel_size, scale, deconv))\n",
    "    return params[::-1] if deconv else params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(params, x):\n",
    "    # x: (batch, height, width, channels)\n",
    "    z = conv(params, x)\n",
    "    z = z.reshape(z.shape[0], -1)\n",
    "    return z\n",
    "\n",
    "def decoder(params, z):\n",
    "    # z: (batch, latent_dim)\n",
    "    s = int(np.sqrt(z.shape[1] / (cfg['channels'] * 2 ** cfg['layers'])))\n",
    "    z = z.reshape(z.shape[0], s, s, cfg['channels'] * 2 ** cfg['layers'])\n",
    "    z = deconv(params, z)\n",
    "    return z\n",
    "\n",
    "\n",
    "def init_linear_params(rng, in_dim, out_dim, scale):\n",
    "    key1, key2 = jax.random.split(rng, 2)\n",
    "    w_shape = (in_dim, out_dim)\n",
    "    b_shape = (out_dim,)\n",
    "    w = scale * jax.random.normal(key1, w_shape)\n",
    "    b = scale * jax.random.normal(key2, b_shape)\n",
    "    return w, b\n",
    "\n",
    "def forward_linear(params, x):\n",
    "    w, b = params\n",
    "    return x @ w + b\n",
    "\n",
    "def init_fn(rng, cfg):\n",
    "    # al 3s are for RGB channels\n",
    "    latent_dim = cfg['channels'] * 2 ** cfg['layers']* (cfg['image_size']// cfg['stride']** cfg['layers']) ** 2  # make stride dependent\n",
    "    rng, key1, key2, key3, key4, key5 = jax.random.split(rng, 6)\n",
    "    params = {\n",
    "        'encoder_conv': init_conv_layers(key1, cfg['channels'], cfg['kernel_size'], cfg['layers'], cfg['scale']),\n",
    "        'decoder_fc': init_linear_params(key2, cfg['embed_dim'], latent_dim, cfg['scale']),\n",
    "        'decoder_conv': init_conv_layers(key3, cfg['channels'], cfg['kernel_size'], cfg['layers'], cfg['scale'], deconv=True),\n",
    "        'linear_mu': init_linear_params(key4, latent_dim, cfg['embed_dim'], cfg['scale']),\n",
    "        'linear_logvar': init_linear_params(key5, latent_dim, cfg['embed_dim'], cfg['scale']),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "\n",
    "def apply_fn(params, x, rng=None):\n",
    "    # x: (batch, height, width, channels)\n",
    "    z = encoder(params['encoder_conv'], x)\n",
    "    mu, logvar = forward_linear(params['linear_mu'], z), forward_linear(params['linear_logvar'], z)\n",
    "    z = reparametrize(mu, logvar, rng) if rng is not None else mu\n",
    "    print(z.mean(), z.std())\n",
    "    z = forward_linear(params['decoder_fc'], z)\n",
    "    z = jax.nn.gelu(z)\n",
    "    x_hat = decoder(params['decoder_conv'], z)\n",
    "    x_hat = jax.nn.sigmoid(x_hat)\n",
    "    return x_hat, mu, logvar\n",
    "\n",
    "\n",
    "def reparametrize(mu, logvar, rng):\n",
    "    # mu, logvar: (batch, latent_dim)\n",
    "    std = jnp.exp(0.5 * logvar)\n",
    "    eps = jax.random.normal(rng, std.shape)\n",
    "    return mu + eps * std\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss_fn(params, x, rng):\n",
    "    # x: (batch, height, width, channels)\n",
    "    x_hat, mu, logvar = apply_fn(params, x, rng)\n",
    "    kl_loss = kl_divergence(mu, logvar) * cfg['beta']\n",
    "    recon_loss = jnp.mean((x - x_hat) ** 2, axis=(1, 2, 3)).mean()\n",
    "    return recon_loss + kl_loss\n",
    "\n",
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * jnp.sum(1 + logvar - mu ** 2 - jnp.exp(logvar), axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, key = jax.random.split(jax.random.PRNGKey(0))\n",
    "params = init_fn(rng, cfg)\n",
    "n_params = syrkis.training.n_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optax.adamw(cfg['lr'])\n",
    "opt_state = opt.init(params)\n",
    "grad_fn = jax.value_and_grad(loss_fn)\n",
    "\n",
    "@jit\n",
    "def update_fn(params, x, opt_state, rng):\n",
    "    loss, grads = grad_fn(params, x, rng)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return loss, params, opt_state\n",
    "\n",
    "# eval_batch = next(loader)[2]\n",
    "eval_batch = mnist[0]\n",
    "generation_batch = jax.random.normal(key, (encoder(params['encoder_conv'], eval_batch).shape))\n",
    "def train_loop(params, opt_state, rng):\n",
    "    # toc = time.time()\n",
    "    for epoch in range(1, cfg['epochs']):\n",
    "        # tic = time.time()\n",
    "        for i in range(8_000 // cfg['batch_size']):\n",
    "            #_, _, img = next(loader)\n",
    "            img = mnist[i]\n",
    "            rng, key = jax.random.split(rng)\n",
    "            loss, params, opt_state = update_fn(params, img, opt_state, key)\n",
    "            eval_imgs = apply_fn(params, eval_batch)[0]\n",
    "            gen_imgs = decoder(params['decoder_conv'], generation_batch)\n",
    "            imgs = jnp.concatenate((eval_imgs[:6], eval_batch[:6], gen_imgs[:6]), axis=0)\n",
    "            syrkis.training.plot_multiples(imgs, n_rows=3, info_bar=[\n",
    "                f\"mse loss : {loss:.3f}\",\n",
    "                f\"embed_dim : {cfg['embed_dim']}\",\n",
    "                f\"n_layers : {cfg['layers']}\",\n",
    "                f\"epoch : {epoch + 1}\",\n",
    "                f\"batch : {i + 1}\",\n",
    "                f\"params : {n_params:,}\",\n",
    "                # f\"eta : {(time.time() - tic) * (8_000 // cfg['batch_size'] - i) / 60:.2f} min\",\n",
    "                ])\n",
    "        # toc = time.time()\n",
    "    return params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(params, opt_state, rng)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
